# ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥ ì˜ì—­ ë¶„ì„

## ğŸ“Š ìš”ì•½

GitHub Feedback Analysis í”„ë¡œì íŠ¸ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•œ ê²°ê³¼, **7ê°œì˜ ì£¼ìš” ê°œì„  ì˜ì—­**ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ëŠ” ì´ë¯¸ ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ì™€ ìºì‹±ì„ ì˜ í™œìš©í•˜ê³  ìˆì§€ë§Œ, ì¶”ê°€ ìµœì í™”ë¥¼ í†µí•´ **30-50% ì„±ëŠ¥ í–¥ìƒ**ì´ ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.

---

## ğŸ¯ ì£¼ìš” ê°œì„  ì˜ì—­

### 1. LLM ë¶„ì„ ë³‘ë ¬í™” ê°•í™” â­â­â­

**í˜„ì¬ ìƒíƒœ:**
```python
# llm.py:913-920
with ThreadPoolExecutor(max_workers=2) as executor:
    comm_future = executor.submit(self.complete, comm_messages, ...)
    code_future = executor.submit(self.complete, code_messages, ...)
```

**ë¬¸ì œì :**
- `analyze_personal_development`ì—ì„œ 2ê°œì˜ ì›Œì»¤ë§Œ ì‚¬ìš©
- ë‹¤ë¥¸ LLM ë¶„ì„ ë©”ì„œë“œë“¤(commit messages, PR titles, review tone, issue quality)ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë  ê°€ëŠ¥ì„±
- `max_workers_llm_analysis: 4`ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ ì™„ì „íˆ í™œìš©ë˜ì§€ ì•ŠìŒ

**ê°œì„  ë°©ì•ˆ:**
```python
# constants.py
PARALLEL_CONFIG = {
    'max_workers_llm_analysis': 6,  # 4 â†’ 6 ì¦ê°€
    # ...
}

# analyzer.py ë˜ëŠ” í˜¸ì¶œ ì½”ë“œì—ì„œ
def analyze_all_feedback_parallel(self, ...):
    """ëª¨ë“  í”¼ë“œë°± ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
    with ThreadPoolExecutor(max_workers=6) as executor:
        futures = {
            'commits': executor.submit(llm_client.analyze_commit_messages, commits),
            'pr_titles': executor.submit(llm_client.analyze_pr_titles, pr_titles),
            'review_tone': executor.submit(llm_client.analyze_review_tone, reviews),
            'issue_quality': executor.submit(llm_client.analyze_issue_quality, issues),
            'personal_dev': executor.submit(llm_client.analyze_personal_development, ...),
            # ... ë‹¤ë¥¸ ë¶„ì„ ì‘ì—…ë“¤
        }

        results = {}
        for key, future in futures.items():
            try:
                results[key] = future.result(timeout=180)
            except TimeoutError:
                logger.warning(f"{key} analysis timed out")
                results[key] = None

        return results
```

**ì˜ˆìƒ íš¨ê³¼:**
- LLM ë¶„ì„ ë‹¨ê³„ì—ì„œ **40-60% ì‹œê°„ ë‹¨ì¶•** (5ê°œ ë¶„ì„ì´ ìˆœì°¨ì ì´ë©´ 5x â†’ ë³‘ë ¬ì´ë©´ 1x)
- ì „ì²´ ì›Œí¬í”Œë¡œìš°ì—ì„œ **20-30% ì‹œê°„ ë‹¨ì¶•**

---

### 2. ë°ì´í„° ìˆ˜ì§‘ ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€ â­â­â­

**í˜„ì¬ ìƒíƒœ:**
```python
# constants.py:249-252
PARALLEL_CONFIG = {
    'max_workers_data_collection': 3,  # Phase 1
    'max_workers_pr_data': 2,          # Phase 2
    # ...
}
```

**ë¬¸ì œì :**
- ë°ì´í„° ìˆ˜ì§‘ Phase 1ì—ì„œ 3ê°œ ì›Œì»¤ë§Œ ì‚¬ìš© (commits, PRs, issues)
- PR ë°ì´í„° ì²˜ë¦¬ Phase 2ì—ì„œ 2ê°œ ì›Œì»¤ë§Œ ì‚¬ìš©
- ëŒ€ë¶€ë¶„ì˜ ì‹œìŠ¤í…œì—ì„œ ë” ë§ì€ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥

**ê°œì„  ë°©ì•ˆ:**
```python
# constants.py
PARALLEL_CONFIG = {
    'max_workers_data_collection': 5,  # 3 â†’ 5 ì¦ê°€
    'max_workers_pr_data': 4,          # 2 â†’ 4 ì¦ê°€
    'max_workers_pr_fetch': 8,         # 5 â†’ 8 ì¦ê°€ (pr_collector.py:122)
    # ...
}
```

**ë™ì  ì›Œì»¤ ìˆ˜ ì„¤ì • (ì„ íƒì ):**
```python
import os

def get_optimal_workers(task_type: str) -> int:
    """CPU ì½”ì–´ ìˆ˜ì— ê¸°ë°˜í•œ ìµœì  ì›Œì»¤ ìˆ˜ ê³„ì‚°"""
    cpu_count = os.cpu_count() or 4

    if task_type == 'io_bound':  # API í˜¸ì¶œ, LLM
        return min(cpu_count * 2, 10)  # I/O boundëŠ” 2x
    elif task_type == 'cpu_bound':  # ë¶„ì„, ê³„ì‚°
        return cpu_count
    else:
        return 4
```

**ì˜ˆìƒ íš¨ê³¼:**
- Phase 1 ë°ì´í„° ìˆ˜ì§‘: **15-25% ì‹œê°„ ë‹¨ì¶•**
- Phase 2 PR ì²˜ë¦¬: **25-40% ì‹œê°„ ë‹¨ì¶•**

---

### 3. API ìºì‹œ ì „ëµ ìµœì í™” â­â­

**í˜„ì¬ ìƒíƒœ:**
```python
# api_client.py:91, constants.py:387
cache_expire_after=3600,  # ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì— 1ì‹œê°„
```

**ë¬¸ì œì :**
- ëª¨ë“  API ì—”ë“œí¬ì¸íŠ¸ì— ë™ì¼í•œ ìºì‹œ ë§Œë£Œ ì‹œê°„ ì ìš©
- Commits/tagsëŠ” ê±°ì˜ ë³€ê²½ë˜ì§€ ì•ŠìŒ â†’ ë” ê¸¸ê²Œ ìºì‹± ê°€ëŠ¥
- Issues/PRsëŠ” ìì£¼ ë³€ê²½ë¨ â†’ ì§§ì€ ìºì‹±ì´ ë” ì ì ˆí•  ìˆ˜ ìˆìŒ

**ê°œì„  ë°©ì•ˆ:**
```python
# api_client.py
class GitHubApiClient:
    # ì—”ë“œí¬ì¸íŠ¸ë³„ ìºì‹œ TTL ë§¤í•‘
    CACHE_TTL_MAP = {
        'commits': 86400,      # 24ì‹œê°„ (ì»¤ë°‹ì€ ë³€ê²½ë˜ì§€ ì•ŠìŒ)
        'tags': 86400,         # 24ì‹œê°„
        'branches': 7200,      # 2ì‹œê°„
        'pulls': 1800,         # 30ë¶„ (ìì£¼ ë³€ê²½)
        'issues': 1800,        # 30ë¶„
        'reviews': 3600,       # 1ì‹œê°„
        'default': 3600,       # 1ì‹œê°„
    }

    def _get_cache_ttl(self, path: str) -> int:
        """ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œì— ê¸°ë°˜í•œ ìºì‹œ TTL ë°˜í™˜"""
        for key, ttl in self.CACHE_TTL_MAP.items():
            if key in path:
                return ttl
        return self.CACHE_TTL_MAP['default']

    def _get_session(self) -> requests.Session:
        if self.session is None:
            if self.enable_cache:
                # URL íŒ¨í„´ë³„ë¡œ ë‹¤ë¥¸ TTL ì ìš©
                from requests_cache import CachedSession
                from requests_cache.policy import ExpirationTime

                self.session = CachedSession(
                    cache_name=str(cache_path),
                    backend="sqlite",
                    expire_after=self._build_expiry_map(),
                    allowable_codes=[200, 301, 302],
                    allowable_methods=["GET", "HEAD"],
                )
```

**ëŒ€ì•ˆ - ìºì‹œ ì›Œë° (Cache Warming):**
```python
def warm_cache(self, repo: str, months: int):
    """ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ìºì‹±"""
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸ ì‚¬ì „ ë¡œë”©
    endpoints = [
        f"repos/{repo}/commits",
        f"repos/{repo}/pulls",
        f"repos/{repo}/branches",
    ]

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(self.request_list, ep) for ep in endpoints]
        # ê²°ê³¼ëŠ” ìºì‹œì— ì €ì¥ë¨
```

**ì˜ˆìƒ íš¨ê³¼:**
- ìºì‹œ íˆíŠ¸ìœ¨ **10-20% ì¦ê°€**
- ë°˜ë³µ ì‹¤í–‰ ì‹œ **15-25% ì‹œê°„ ë‹¨ì¶•**

---

### 4. Phase 1/2 ì˜¤ë²„ë˜í•‘ ì‹¤í–‰ â­â­

**í˜„ì¬ ìƒíƒœ:**
```python
# collector.py:116-119
phase1_result = self._collect_phase_one(repo, since, filters, author)
pull_request_examples, reviews = self._collect_phase_two(
    repo, since, filters, phase1_result.pr_metadata
)
```

**ë¬¸ì œì :**
- Phase 1ì´ ì™„ì „íˆ ëë‚œ í›„ì—ë§Œ Phase 2 ì‹œì‘
- Phase 2ëŠ” PR ë©”íƒ€ë°ì´í„°ë§Œ í•„ìš”í•˜ë¯€ë¡œ Phase 1ì˜ ì¼ë¶€ ì™„ë£Œ í›„ ì‹œì‘ ê°€ëŠ¥

**ê°œì„  ë°©ì•ˆ:**
```python
# collector.py
def collect(self, repo: str, months: int, filters: Optional[AnalysisFilters] = None, author: Optional[str] = None):
    """Overlapping Phase 1 and Phase 2 execution"""

    from queue import Queue
    pr_metadata_queue = Queue()

    def phase_one_with_streaming():
        """Phase 1 ì‹¤í–‰í•˜ë©´ì„œ PR ë©”íƒ€ë°ì´í„°ë¥¼ íì— ìŠ¤íŠ¸ë¦¬ë°"""
        # commits, issues ìˆ˜ì§‘
        commits = self.commit_collector.count_commits(...)
        issues = self.issue_collector.count_issues(...)

        # PR ìˆ˜ì§‘í•˜ë©´ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ íì— ì „ë‹¬
        pull_requests, pr_metadata = self.pr_collector.list_pull_requests(...)
        pr_metadata_queue.put(pr_metadata)  # Phase 2ê°€ ì‹œì‘í•  ìˆ˜ ìˆê²Œ ì‹ í˜¸

        return commits, pull_requests, issues

    def phase_two_consumer():
        """íì—ì„œ PR ë©”íƒ€ë°ì´í„°ë¥¼ ë°›ì•„ Phase 2 ì‹œì‘"""
        pr_metadata = pr_metadata_queue.get()  # Phase 1 PR ìˆ˜ì§‘ ì™„ë£Œ ëŒ€ê¸°
        return self._collect_phase_two(repo, since, filters, pr_metadata)

    # ë‘ Phaseë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=2) as executor:
        phase1_future = executor.submit(phase_one_with_streaming)
        phase2_future = executor.submit(phase_two_consumer)

        commits, pull_requests, issues = phase1_future.result()
        pull_request_examples, reviews = phase2_future.result()
```

**ì˜ˆìƒ íš¨ê³¼:**
- Phase ê°„ ëŒ€ê¸° ì‹œê°„ **ì œê±°**
- ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ **10-15% ì‹œê°„ ë‹¨ì¶•**

---

### 5. SQLite ìºì‹œ DB ìµœì í™” â­

**í˜„ì¬ ìƒíƒœ:**
- SQLite ìºì‹œ ì‚¬ìš© ì¤‘ì´ì§€ë§Œ ì¸ë±ì‹± ì •ë³´ ì—†ìŒ
- ì—°ê²° í’€ë§ ì„¤ì • ì—†ìŒ

**ê°œì„  ë°©ì•ˆ:**
```python
# api_client.py ë˜ëŠ” ìƒˆë¡œìš´ cache_optimizer.py
def optimize_cache_db(cache_path: Path):
    """ìºì‹œ DBì— ì¸ë±ìŠ¤ ì¶”ê°€ ë° ìµœì í™”"""
    import sqlite3

    conn = sqlite3.connect(cache_path)
    cursor = conn.cursor()

    # ì¸ë±ìŠ¤ ì¶”ê°€ (requests-cache í…Œì´ë¸” êµ¬ì¡°ì— ë§ê²Œ)
    try:
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_cache_key
            ON responses(key)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_cache_expires
            ON responses(expires)
        """)

        # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
        cursor.execute("DELETE FROM responses WHERE expires < datetime('now')")

        # VACUUMìœ¼ë¡œ DB íŒŒì¼ í¬ê¸° ìµœì í™”
        cursor.execute("VACUUM")

        conn.commit()
    except sqlite3.OperationalError as e:
        logger.warning(f"Cache optimization failed: {e}")
    finally:
        conn.close()

# ì •ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
def maintain_cache():
    """ìºì‹œ ìœ ì§€ë³´ìˆ˜ - ë§Œë£Œëœ í•­ëª© ì œê±°"""
    cache_path = Path.home() / ".cache" / "github_feedback" / "api_cache.sqlite"
    if cache_path.exists():
        optimize_cache_db(cache_path)
```

**SQLite ì„±ëŠ¥ íŠœë‹:**
```python
# CachedSession ìƒì„± ì‹œ
import sqlite3
from requests_cache import CachedSession

# SQLite ì—°ê²° ì˜µì…˜ ìµœì í™”
def create_optimized_cache(cache_path: str):
    session = CachedSession(
        cache_name=cache_path,
        backend="sqlite",
        expire_after=3600,
    )

    # ë‚´ë¶€ SQLite ì—°ê²°ì— ì„±ëŠ¥ ì„¤ì • ì ìš©
    if hasattr(session.cache, 'connection'):
        conn = session.cache.connection()
        conn.execute("PRAGMA journal_mode=WAL")       # Write-Ahead Logging
        conn.execute("PRAGMA synchronous=NORMAL")      # ë””ìŠ¤í¬ ë™ê¸°í™” ì™„í™”
        conn.execute("PRAGMA cache_size=-64000")       # 64MB ë©”ëª¨ë¦¬ ìºì‹œ
        conn.execute("PRAGMA temp_store=MEMORY")       # ì„ì‹œ í…Œì´ë¸”ì„ ë©”ëª¨ë¦¬ì—

    return session
```

**ì˜ˆìƒ íš¨ê³¼:**
- ìºì‹œ ì¡°íšŒ **20-30% ì†ë„ í–¥ìƒ**
- ìºì‹œ ì €ì¥ **15-25% ì†ë„ í–¥ìƒ**
- DB íŒŒì¼ í¬ê¸° **10-20% ê°ì†Œ**

---

### 6. ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™” â­

**í˜„ì¬ ìƒíƒœ:**
- ëŒ€ëŸ‰ì˜ PR íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë©”ëª¨ë¦¬ì— ëª¨ë‘ ë¡œë“œ
- í° ì €ì¥ì†Œì—ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±

**ê°œì„  ë°©ì•ˆ:**
```python
# pr_collector.py
def collect_pull_request_details_streaming(self, repo: str, number: int) -> PullRequestReviewBundle:
    """PR ìƒì„¸ ì •ë³´ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""

    # íŒŒì¼ ìˆ˜ê°€ ë§ì€ ê²½ìš° í˜ì´ì§• ì²˜ë¦¬
    files_count = self.api_client.request_json(f"repos/{repo}/pulls/{number}")["changed_files"]

    if files_count > 100:  # íŒŒì¼ì´ ë§ìœ¼ë©´ ìŠ¤íŠ¸ë¦¬ë°
        files = []
        page = 1
        per_page = 30

        while len(files) < files_count:
            page_files = self.api_client.request_list(
                f"repos/{repo}/pulls/{number}/files",
                {"page": page, "per_page": per_page}
            )

            # í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            for entry in page_files:
                files.append(
                    PullRequestFile(
                        filename=entry["filename"],
                        status=entry["status"],
                        additions=entry.get("additions", 0),
                        deletions=entry.get("deletions", 0),
                        changes=entry.get("changes", 0),
                        patch=None,  # í° patchëŠ” ì œì™¸ (í•„ìš”ì‹œì—ë§Œ ë¡œë“œ)
                    )
                )

            if len(page_files) < per_page:
                break
            page += 1
    else:
        # íŒŒì¼ì´ ì ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
        files_payload = self.api_client.request_all(...)
        files = [PullRequestFile(...) for entry in files_payload]

    return PullRequestReviewBundle(files=files, ...)
```

**ì œë„ˆë ˆì´í„° í™œìš©:**
```python
def iter_commits(self, repo: str, since: datetime, filters: AnalysisFilters):
    """ì»¤ë°‹ì„ ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½"""
    for branch in self._get_branches_to_process(repo, filters):
        params = build_commits_params(sha=branch, since=since.isoformat())

        page = 1
        while True:
            commits_page = self.api_client.request_list(
                f"repos/{repo}/commits",
                params | {"page": page, "per_page": 100}
            )

            if not commits_page:
                break

            for commit in commits_page:
                yield commit  # í•œ ë²ˆì— í•˜ë‚˜ì”© ë°˜í™˜

            page += 1
```

**ì˜ˆìƒ íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ **30-50% ê°ì†Œ**
- í° ì €ì¥ì†Œì—ì„œ OOM(Out Of Memory) ì˜¤ë¥˜ ë°©ì§€

---

### 7. GraphQL API í™œìš© (ì„ íƒì ) â­

**í˜„ì¬ ìƒíƒœ:**
- ëª¨ë“  ë°ì´í„°ë¥¼ REST APIë¡œ ìˆ˜ì§‘
- ì—¬ëŸ¬ ë²ˆì˜ API í˜¸ì¶œ í•„ìš”

**ê°œì„  ë°©ì•ˆ:**
```python
# graphql_client.py (ìƒˆ íŒŒì¼)
def fetch_pr_with_reviews_graphql(repo: str, pr_numbers: List[int]) -> List[Dict]:
    """GraphQLë¡œ PRê³¼ ë¦¬ë·°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°"""

    query = """
    query($owner: String!, $name: String!, $numbers: [Int!]!) {
      repository(owner: $owner, name: $name) {
        pullRequests(first: 100, numbers: $numbers) {
          nodes {
            number
            title
            body
            author { login }
            additions
            deletions
            changedFiles
            reviews(first: 100) {
              nodes {
                body
                author { login }
              }
            }
            comments(first: 100) {
              nodes {
                body
                author { login }
              }
            }
            files(first: 100) {
              nodes {
                path
                additions
                deletions
              }
            }
          }
        }
      }
    }
    """

    owner, name = repo.split('/')
    response = requests.post(
        "https://api.github.com/graphql",
        json={
            "query": query,
            "variables": {
                "owner": owner,
                "name": name,
                "numbers": pr_numbers
            }
        },
        headers={"Authorization": f"Bearer {pat}"}
    )

    return response.json()["data"]["repository"]["pullRequests"]["nodes"]
```

**REST vs GraphQL ë¹„êµ:**
- **REST**: PR 10ê°œ + ë¦¬ë·° 10ê°œ + ì½”ë©˜íŠ¸ 10ê°œ = **30 API í˜¸ì¶œ**
- **GraphQL**: **1 API í˜¸ì¶œ**ë¡œ ëª¨ë“  ë°ì´í„° íšë“

**ì˜ˆìƒ íš¨ê³¼:**
- API í˜¸ì¶œ íšŸìˆ˜ **70-90% ê°ì†Œ**
- ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œ **50-70% ê°ì†Œ**
- Rate limit ì†Œë¹„ **ëŒ€í­ ì ˆê°**

---

## ğŸ“ˆ êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ì˜ˆìƒ íš¨ê³¼

| ìš°ì„ ìˆœìœ„ | ê°œì„  ì˜ì—­ | ë‚œì´ë„ | ì˜ˆìƒ íš¨ê³¼ | êµ¬í˜„ ì‹œê°„ |
|---------|----------|--------|----------|---------|
| 1 | LLM ë¶„ì„ ë³‘ë ¬í™” | â­â­ | 20-30% ë‹¨ì¶• | 2-4ì‹œê°„ |
| 2 | ë°ì´í„° ìˆ˜ì§‘ ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€ | â­ | 15-25% ë‹¨ì¶• | 1-2ì‹œê°„ |
| 3 | API ìºì‹œ ì „ëµ ìµœì í™” | â­â­â­ | 15-25% ë‹¨ì¶• | 4-6ì‹œê°„ |
| 4 | SQLite ìºì‹œ ìµœì í™” | â­â­ | 10-15% ë‹¨ì¶• | 2-3ì‹œê°„ |
| 5 | Phase ì˜¤ë²„ë˜í•‘ | â­â­â­ | 10-15% ë‹¨ì¶• | 3-5ì‹œê°„ |
| 6 | ë©”ëª¨ë¦¬ ìµœì í™” | â­â­ | ë©”ëª¨ë¦¬ ì ˆì•½ | 3-4ì‹œê°„ |
| 7 | GraphQL API | â­â­â­â­ | 50-70% ë‹¨ì¶• | 8-12ì‹œê°„ |

**ì „ì²´ ì˜ˆìƒ íš¨ê³¼:** ìš°ì„ ìˆœìœ„ 1-4ë¥¼ êµ¬í˜„í•˜ë©´ **ì „ì²´ ì‹¤í–‰ ì‹œê°„ 40-60% ë‹¨ì¶•** ê°€ëŠ¥

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„  (5ë¶„ ë‚´)

**1. ì›Œì»¤ ìˆ˜ ì¦ê°€:**
```python
# github_feedback/constants.py ìˆ˜ì •
PARALLEL_CONFIG = {
    'max_workers_data_collection': 5,  # 3 â†’ 5
    'max_workers_pr_data': 4,          # 2 â†’ 4
    'max_workers_llm_analysis': 6,     # 4 â†’ 6
    'max_workers_pr_fetch': 8,         # 5 â†’ 8
    # ... ë‚˜ë¨¸ì§€ ë™ì¼
}
```

**2. ìºì‹œ ë§Œë£Œ ì‹œê°„ ì¦ê°€ (ì•ˆì •ì ì¸ ë°ì´í„°ìš©):**
```python
# github_feedback/constants.py ìˆ˜ì •
API_DEFAULTS = {
    'cache_expire_seconds': 7200,  # 3600 â†’ 7200 (2ì‹œê°„)
    # ... ë‚˜ë¨¸ì§€ ë™ì¼
}
```

### ì¤‘ê¸° ê°œì„  (1-2ì¼)

**LLM ë¶„ì„ ë³‘ë ¬í™” êµ¬í˜„**
- `analyzer.py`ì— `analyze_all_feedback_parallel()` ë©”ì„œë“œ ì¶”ê°€
- ê¸°ì¡´ ìˆœì°¨ í˜¸ì¶œì„ ë³‘ë ¬ í˜¸ì¶œë¡œ ë³€ê²½

### ì¥ê¸° ê°œì„  (1-2ì£¼)

**GraphQL API ë§ˆì´ê·¸ë ˆì´ì…˜**
- `graphql_client.py` ëª¨ë“ˆ ìƒì„±
- í•µì‹¬ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ì„ GraphQLë¡œ ì „í™˜
- ê¸°ì¡´ REST APIëŠ” fallbackìœ¼ë¡œ ìœ ì§€

---

## ğŸ” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ê°œì„  íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­:

```python
# performance_monitor.py (ìƒˆ íŒŒì¼)
import time
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Dict

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    phase_timings: Dict[str, float]
    api_calls: int
    cache_hits: int
    cache_misses: int
    memory_peak_mb: float

@contextmanager
def measure_phase(phase_name: str, metrics: PerformanceMetrics):
    """ê° ë‹¨ê³„ì˜ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
    start = time.time()
    try:
        yield
    finally:
        duration = time.time() - start
        metrics.phase_timings[phase_name] = duration
        logger.info(f"{phase_name} completed in {duration:.2f}s")

# ì‚¬ìš© ì˜ˆì‹œ
metrics = PerformanceMetrics(phase_timings={}, ...)

with measure_phase("data_collection", metrics):
    result = collector.collect(repo, months, filters)

with measure_phase("llm_analysis", metrics):
    analysis = analyzer.analyze(result)

print(f"Total time: {sum(metrics.phase_timings.values()):.2f}s")
print(f"Cache hit rate: {metrics.cache_hits / (metrics.cache_hits + metrics.cache_misses):.1%}")
```

---

## ğŸ’¡ ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### Rate Limiting ê´€ë¦¬
```python
# ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€ ì‹œ GitHub API rate limit ì£¼ì˜
# api_client.pyì— rate limit ì²´í¬ ì¶”ê°€

def check_rate_limit(self) -> Dict[str, int]:
    """í˜„ì¬ rate limit ìƒíƒœ í™•ì¸"""
    response = self.request_json("/rate_limit")
    core = response["resources"]["core"]

    return {
        "remaining": core["remaining"],
        "limit": core["limit"],
        "reset_at": datetime.fromtimestamp(core["reset"])
    }

def wait_if_rate_limited(self):
    """Rate limitì— ê°€ê¹Œìš°ë©´ ëŒ€ê¸°"""
    status = self.check_rate_limit()

    if status["remaining"] < 100:  # 100ê°œ ë¯¸ë§Œì´ë©´ ëŒ€ê¸°
        wait_seconds = (status["reset_at"] - datetime.now()).total_seconds()
        logger.warning(f"Rate limit low, waiting {wait_seconds:.0f}s")
        time.sleep(wait_seconds + 1)
```

### ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
```python
# ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€ë¡œ ì‹¤íŒ¨ í™•ë¥  ì¦ê°€ â†’ retry ë¡œì§ ê°•í™”
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def resilient_api_call(self, path: str, params: dict):
    """ì¬ì‹œë„ ë¡œì§ì´ ê°•í™”ëœ API í˜¸ì¶œ"""
    return self.api_client.request_list(path, params)
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [GitHub REST API Rate Limiting](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting)
- [GitHub GraphQL API](https://docs.github.com/en/graphql)
- [Python ThreadPoolExecutor Best Practices](https://docs.python.org/3/library/concurrent.futures.html)
- [SQLite Performance Tuning](https://www.sqlite.org/pragma.html)
- [requests-cache Documentation](https://requests-cache.readthedocs.io/)

---

## ğŸ“ ê²°ë¡ 

ì´ í”„ë¡œì íŠ¸ëŠ” ì´ë¯¸ ë§ì€ ìµœì í™”ê°€ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- âœ… ThreadPoolExecutor ë³‘ë ¬ ì²˜ë¦¬
- âœ… SQLite ìºì‹±
- âœ… Early stopping ì¡°ê±´
- âœ… ì¤‘ë³µ ì œê±° ë¡œì§

ìœ„ì—ì„œ ì œì•ˆí•œ ê°œì„ ì‚¬í•­ë“¤ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í•˜ë©´:
1. **ì¦‰ì‹œ ê°œì„  (ìš°ì„ ìˆœìœ„ 1-2)**: ì„¤ì • ë³€ê²½ë§Œìœ¼ë¡œ **15-25% ì„±ëŠ¥ í–¥ìƒ**
2. **ì¤‘ê¸° ê°œì„  (ìš°ì„ ìˆœìœ„ 3-5)**: ì½”ë“œ ë¦¬íŒ©í† ë§ìœ¼ë¡œ **ì¶”ê°€ 20-30% í–¥ìƒ**
3. **ì¥ê¸° ê°œì„  (ìš°ì„ ìˆœìœ„ 6-7)**: ì•„í‚¤í…ì²˜ ë³€ê²½ìœ¼ë¡œ **ì´ 60-80% í–¥ìƒ ê°€ëŠ¥**

ê°€ì¥ ë¹ ë¥¸ íš¨ê³¼ë¥¼ ìœ„í•´ì„œëŠ” **ìš°ì„ ìˆœìœ„ 1-2ë²ˆì„ ë¨¼ì € êµ¬í˜„**í•˜ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.
