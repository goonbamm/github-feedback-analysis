"""Metric calculation logic for GitHub feedback analysis."""

from __future__ import annotations

import math
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Dict, List, NamedTuple, Optional

from .award_strategies import AwardCalculator
from .console import Console
from .constants import (
    ACTIVITY_THRESHOLDS,
    COLLECTION_LIMITS,
    CONSISTENCY_THRESHOLDS,
    CRITIQUE_THRESHOLDS,
    DISPLAY_LIMITS,
    TREND_THRESHOLDS,
)
from .models import (
    AnalysisStatus,
    CollectionResult,
    MetricSnapshot,
    DetailedFeedbackSnapshot,
    CommitMessageFeedback,
    PRTitleFeedback,
    PullRequestSummary,
    ReviewToneFeedback,
    IssueFeedback,
    MonthlyTrend,
    MonthlyTrendInsights,
    TechStackAnalysis,
    CollaborationNetwork,
    YearEndReview,
    PersonalDevelopmentAnalysis,
    StrengthPoint,
    ImprovementArea,
    GrowthIndicator,
    WitchCritique,
    WitchCritiqueItem,
)
from .retrospective import RetrospectiveAnalyzer

console = Console()


# ============================================================================
# Helper Classes for Analysis
# ============================================================================

class ActivityMessageBuilder:
    """Helper class for building activity-based messages with threshold checks."""

    @staticmethod
    def build_if_exceeds(
        value: int | float,
        threshold: int | float,
        message_template: str,
        *format_args
    ) -> Optional[str]:
        """Build a message if value exceeds threshold.

        Args:
            value: The value to check
            threshold: The threshold to compare against
            message_template: Template string with placeholders
            *format_args: Arguments to format the template

        Returns:
            Formatted message if value > threshold, None otherwise
        """
        if value > threshold:
            return message_template.format(*format_args)
        return None

    @staticmethod
    def build_messages_from_checks(
        checks: List[tuple[int | float, int | float, str, tuple]]
    ) -> List[str]:
        """Build messages from a list of threshold checks.

        Args:
            checks: List of (value, threshold, template, args) tuples

        Returns:
            List of messages where threshold was exceeded
        """
        messages = []
        for value, threshold, template, args in checks:
            msg = ActivityMessageBuilder.build_if_exceeds(value, threshold, template, *args)
            if msg:
                messages.append(msg)
        return messages


class InsightExtractor:
    """Helper class for extracting insights from PR collections."""

    @staticmethod
    def filter_prs_by_keywords(prs: list, keywords: list[str]) -> list:
        """Filter pull requests by keywords in their titles.

        Args:
            prs: List of pull requests to filter
            keywords: List of keywords to search for in titles (case-insensitive)

        Returns:
            Filtered list of pull requests
        """
        return [pr for pr in prs if any(kw in pr.title.lower() for kw in keywords)]

    @staticmethod
    def categorize_prs_by_keywords(prs: list, keyword_groups: dict[str, list[str]]) -> dict[str, list]:
        """Categorize pull requests by multiple keyword groups in a single pass.

        This is more efficient than calling filter_prs_by_keywords multiple times
        as it only iterates through the PRs once.

        Args:
            prs: List of pull requests to categorize
            keyword_groups: Dictionary mapping category names to keyword lists
                Example: {'doc': ['doc', 'readme'], 'test': ['test']}

        Returns:
            Dictionary mapping category names to filtered PR lists
        """
        # Initialize result dictionary with empty lists
        result = {category: [] for category in keyword_groups}

        # Single pass through all PRs
        for pr in prs:
            pr_title_lower = pr.title.lower()
            for category, keywords in keyword_groups.items():
                if any(kw in pr_title_lower for kw in keywords):
                    result[category].append(pr)

        return result

    @staticmethod
    def extract_keyword_based_insight(
        prs: list,
        keywords: list[str],
        threshold: int,
        message_template: str
    ) -> Optional[str]:
        """Extract insight based on keyword filtering and threshold check.

        Args:
            prs: List of pull requests
            keywords: Keywords to filter by
            threshold: Minimum count for insight
            message_template: Template for the insight message (with {count} placeholder)

        Returns:
            Formatted message if threshold exceeded, None otherwise
        """
        if not prs:
            return None

        filtered_prs = InsightExtractor.filter_prs_by_keywords(prs, keywords)
        if len(filtered_prs) > threshold:
            return message_template.format(count=len(filtered_prs))
        return None


class PeriodFormatter:
    """Format period labels based on month count."""

    # Mapping of common month counts to Korean labels
    LABEL_MAP = {
        3: "ìµœê·¼ 3ê°œì›”",
        6: "ìµœê·¼ 6ê°œì›”",
        12: "ìµœê·¼ 1ë…„",
    }

    @staticmethod
    def format_period(months: int) -> str:
        """Format period label based on month count.

        Args:
            months: Number of months in the period

        Returns:
            Formatted period label in Korean

        Examples:
            >>> PeriodFormatter.format_period(3)
            'ìµœê·¼ 3ê°œì›”'
            >>> PeriodFormatter.format_period(12)
            'ìµœê·¼ 1ë…„'
            >>> PeriodFormatter.format_period(25)
            'ìµœê·¼ 2ë…„ 1ê°œì›”'
        """
        # Check for exact matches first
        if months in PeriodFormatter.LABEL_MAP:
            return PeriodFormatter.LABEL_MAP[months]

        # Handle years and remaining months
        from github_feedback.constants import MONTHS_FOR_YEAR_DISPLAY, MONTHS_PER_YEAR
        if months >= MONTHS_FOR_YEAR_DISPLAY:
            years = months // MONTHS_PER_YEAR
            remaining_months = months % MONTHS_PER_YEAR
            if remaining_months == 0:
                return f"ìµœê·¼ {years}ë…„"
            return f"ìµœê·¼ {years}ë…„ {remaining_months}ê°œì›”"

        # Default to months
        return f"ìµœê·¼ {months}ê°œì›”"


class CollectionStats(NamedTuple):
    """Statistics computed from collection data."""
    month_span: int
    velocity_score: float
    collaboration_score: float
    stability_score: int
    total_activity: int
    period_label: str


@dataclass(slots=True)
class Analyzer:
    """Transform collected data into actionable metrics."""

    web_base_url: str = "https://github.com"

    def compute_metrics(
        self,
        collection: CollectionResult,
        detailed_feedback: Optional[DetailedFeedbackSnapshot] = None,
        monthly_trends_data: Optional[List[Dict]] = None,
        tech_stack_data: Optional[Dict[str, int]] = None,
        collaboration_data: Optional[Dict[str, Any]] = None,
    ) -> MetricSnapshot:
        """Compute derived metrics from the collected artefacts."""

        console.log("Analyzing repository trends", f"repo={collection.repo}")

        stats = self._calculate_scores(collection)

        highlights = self._build_highlights(
            collection,
            stats.period_label,
            stats.month_span,
            stats.velocity_score,
            stats.total_activity,
        )
        spotlight_examples = self._build_spotlight_examples(collection)
        summary = self._build_summary(
            stats.period_label,
            stats.total_activity,
            stats.velocity_score,
            stats.collaboration_score,
            stats.stability_score,
        )
        story_beats = self._build_story_beats(collection, stats.period_label, stats.total_activity)
        awards = self._determine_awards(collection)
        metric_stats = self._build_stats(collection, stats.velocity_score)
        evidence = self._build_evidence(collection)

        # Build year-end specific insights
        monthly_trends = self._build_monthly_trends(monthly_trends_data)
        monthly_insights = self._build_monthly_insights(monthly_trends)
        tech_stack = self._build_tech_stack_analysis(tech_stack_data)
        collaboration = self._build_collaboration_network(collaboration_data)
        year_end_review = self._build_year_end_review(collection, highlights, awards)

        # Generate witch's critique
        witch_critique = self._generate_witch_critique(collection, detailed_feedback)

        # Create initial metrics snapshot
        metrics_snapshot = MetricSnapshot(
            repo=collection.repo,
            months=collection.months,
            generated_at=datetime.now(timezone.utc),
            status=AnalysisStatus.ANALYSED,
            summary=summary,
            stats=metric_stats,
            evidence=evidence,
            highlights=highlights,
            spotlight_examples=spotlight_examples,
            yearbook_story=story_beats,
            awards=awards,
            detailed_feedback=detailed_feedback,
            monthly_trends=monthly_trends,
            monthly_insights=monthly_insights,
            tech_stack=tech_stack,
            collaboration=collaboration,
            year_end_review=year_end_review,
            witch_critique=witch_critique,
            since_date=collection.since_date,
            until_date=collection.until_date,
        )

        # Generate comprehensive retrospective analysis
        console.log("Generating retrospective analysis", f"repo={collection.repo}")
        retrospective_analyzer = RetrospectiveAnalyzer()
        retrospective = retrospective_analyzer.analyze(metrics_snapshot)
        metrics_snapshot.retrospective = retrospective

        return metrics_snapshot

    def _calculate_scores(
        self, collection: CollectionResult
    ) -> CollectionStats:
        month_span = max(collection.months, 1)
        velocity_score = collection.commits / month_span
        collaboration_score = (collection.pull_requests + collection.reviews) / month_span
        stability_score = max(collection.commits - collection.issues, 0)
        total_activity = collection.commits + collection.pull_requests + collection.reviews
        period_label = PeriodFormatter.format_period(collection.months)

        return CollectionStats(
            month_span=month_span,
            velocity_score=velocity_score,
            collaboration_score=collaboration_score,
            stability_score=stability_score,
            total_activity=total_activity,
            period_label=period_label,
        )

    def _build_highlights(
        self,
        collection: CollectionResult,
        period_label: str,
        month_span: int,
        velocity_score: float,
        total_activity: int,
    ) -> List[str]:
        highlights: List[str] = []
        if collection.commits:
            highlights.append(
                f"{period_label}ì— ì´ {collection.commits}íšŒì˜ ì»¤ë°‹ìœ¼ë¡œ ì½”ë“œë¥¼ ë‹¤ë“¬ê³  ì›” í‰ê·  {velocity_score:.1f}íšŒì˜ ê°œì„ ì„ ì´ì–´ê°”ìŠµë‹ˆë‹¤."
            )
        if collection.pull_requests:
            highlights.append(
                f"{collection.pull_requests}ê±´ì˜ Pull Requestë¥¼ ë³‘í•©í•˜ë©° íŒ€ ë°°í¬ ì£¼ê¸°ë¥¼ ì•ˆì •í™”í–ˆê³  ì›” {collection.pull_requests / month_span:.1f}ê±´ì˜ ë¦´ë¦¬ìŠ¤ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤."
            )
        if collection.reviews:
            highlights.append(
                f"{collection.reviews}íšŒì˜ ì½”ë“œ ë¦¬ë·°ë¥¼ í†µí•´ í˜‘ì—… ë¬¸í™”ë¥¼ ê°•í™”í–ˆìŠµë‹ˆë‹¤."
            )
        if collection.issues:
            highlights.append(
                f"í™œë™ ëŒ€ë¹„ {collection.issues}ê±´ì˜ ì´ìŠˆë¡œ ì•ˆì •ì„±ì„ ì§€ì¼°ìŠµë‹ˆë‹¤."
            )
        if not highlights and total_activity == 0:
            highlights.append("ë¶„ì„ ê¸°ê°„ ë™ì•ˆ ëšœë ·í•œ í™œë™ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return highlights

    def _build_spotlight_examples(self, collection: CollectionResult) -> Dict[str, List[str]]:
        spotlight_examples: Dict[str, List[str]] = {}
        if not collection.pull_request_examples:
            return spotlight_examples

        pr_lines = []
        for pr in collection.pull_request_examples[:COLLECTION_LIMITS['pr_examples']]:
            change_volume = pr.additions + pr.deletions
            scale_phrase = f"ë³€ê²½ {change_volume}ì¤„" if change_volume else "ê²½ëŸ‰ ë³€ê²½"
            merged_phrase = (
                f"{pr.merged_at.date().isoformat()} ë³‘í•©"
                if pr.merged_at
                else "ë¯¸ë³‘í•©"
            )
            pr_lines.append(
                f"PR #{pr.number} Â· {pr.title} â€” {pr.author} ({pr.created_at.date().isoformat()}, {merged_phrase}, {scale_phrase}) Â· {pr.html_url}"
            )
        spotlight_examples["pull_requests"] = pr_lines
        return spotlight_examples

    def _build_summary(
        self,
        period_label: str,
        total_activity: int,
        velocity_score: float,
        collaboration_score: float,
        stability_score: int,
    ) -> Dict[str, str]:
        return {
            "velocity": f"Average {velocity_score:.1f} commits per month",
            "collaboration": "{:.1f} combined PRs and reviews per month".format(collaboration_score),
            "stability": f"Net stability score of {stability_score}",
            "growth": f"{period_label} ë™ì•ˆ {total_activity}ê±´ì˜ í™œë™ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
        }

    def _build_story_beats(
        self,
        collection: CollectionResult,
        period_label: str,
        total_activity: int,
    ) -> List[str]:
        story_beats: List[str] = []
        if total_activity:
            story_beats.append(
                f"{period_label} ë™ì•ˆ {collection.repo} ì €ì¥ì†Œì—ì„œ ì´ {total_activity}ê±´ì˜ í™œë™ì„ í¼ì¹˜ë©° ì„±ì¥ ì—”ì§„ì„ ê°€ë™í–ˆìŠµë‹ˆë‹¤."
            )
        else:
            story_beats.append(
                f"{period_label}ì—ëŠ” ì ì‹œ ìˆ¨ì„ ê³ ë¥´ë©° ë‹¤ìŒ ë„ì•½ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤."
            )

        contribution_domains = [
            ("ì»¤ë°‹", collection.commits, "ì§€ì†ì ì¸ ë¦¬íŒ©í„°ë§ê³¼ ê¸°ëŠ¥ í™•ì¥ì„ ì´ëŒì—ˆìŠµë‹ˆë‹¤."),
            ("Pull Request", collection.pull_requests, "í˜‘ì—… ë¦´ë¦¬ìŠ¤ë¥¼ ì£¼ë„í•˜ë©° ë°°í¬ íŒŒì´í”„ë¼ì¸ì„ ì§€ì¼°ìŠµë‹ˆë‹¤."),
            ("ë¦¬ë·°", collection.reviews, "íŒ€ ë™ë£Œë“¤ì˜ ì„±ì¥ì„ ë•ëŠ” ì´˜ì´˜í•œ í”¼ë“œë°±ì„ ì „ë‹¬í–ˆìŠµë‹ˆë‹¤."),
        ]
        top_domain = max(contribution_domains, key=lambda entry: entry[1])
        if top_domain[1]:
            story_beats.append(
                f"ê°€ì¥ ëˆˆì— ëˆ ì˜ì—­ì€ {top_domain[0]} {top_domain[1]}íšŒë¡œ, {top_domain[2]}"
            )

        if collection.pull_request_examples:
            exemplar = collection.pull_request_examples[0]
            merge_phrase = (
                f"{exemplar.merged_at.date().isoformat()} ë³‘í•©"
                if exemplar.merged_at
                else "ì•„ì§ ì§„í–‰ ì¤‘"
            )
            scale = exemplar.additions + exemplar.deletions
            scale_phrase = f"ë³€ê²½ {scale}ì¤„" if scale else "ê²½ëŸ‰ ë³€ê²½"
            story_beats.append(
                "ëŒ€í‘œì‘ìœ¼ë¡œëŠ” PR #{num} `{title}`({author})ê°€ ìˆìŠµë‹ˆë‹¤ â€” {created} ì‘ì„±, {merge} Â· {scale_phrase}.".format(
                    num=exemplar.number,
                    title=exemplar.title,
                    author=exemplar.author,
                    created=exemplar.created_at.date().isoformat(),
                    merge=merge_phrase,
                    scale_phrase=scale_phrase,
                )
            )

        return story_beats

    def _determine_awards(self, collection: CollectionResult) -> List[str]:
        """Determine awards based on collection metrics using Strategy pattern.

        This method delegates award calculation to the AwardCalculator,
        which orchestrates multiple award strategies.

        Args:
            collection: Collection of repository data

        Returns:
            List of award strings
        """
        calculator = AwardCalculator()
        return calculator.determine_awards(collection)

    def _generate_witch_critique(
        self,
        collection: CollectionResult,
        detailed_feedback: Optional[DetailedFeedbackSnapshot] = None,
    ) -> WitchCritique:
        """Generate harsh but constructive critique from the witch.

        This method ALWAYS returns a WitchCritique object. Even when no specific
        issues are found, it provides general improvement suggestions to ensure
        the witch's critique is always present in the report.

        Args:
            collection: Collection of repository data
            detailed_feedback: Optional detailed feedback snapshot

        Returns:
            WitchCritique with harsh but productive feedback (always returns, never None)
        """
        critiques: List[WitchCritiqueItem] = []

        # Check various aspects of development practices
        self._check_commit_message_quality(detailed_feedback, critiques)
        self._check_pr_size(collection, critiques)
        self._check_pr_title_quality(detailed_feedback, critiques)
        self._check_review_quality(collection, detailed_feedback, critiques)
        self._check_activity_consistency(collection, critiques)
        self._check_documentation_culture(collection, critiques)
        self._check_test_coverage(collection, critiques)
        self._check_branch_management(collection, critiques)
        self._check_issue_tracking(collection, critiques)
        self._check_collaboration_diversity(collection, critiques)

        # If no specific critiques, add fallback so witch always appears
        if not critiques:
            critiques.append(self._get_random_general_critique(collection))

        # Create witch critique with opening and closing
        import random
        opening_curses = [
            "ğŸ”® ì, ìˆ˜ì • êµ¬ìŠ¬ì„ ë“¤ì—¬ë‹¤ë³´ë‹ˆ... í , ê°œì„ í•  ê²Œ ì¢€ ë³´ì´ëŠ”êµ°.",
            "ğŸ”® í¬ë¦¬ìŠ¤íƒˆ ë³¼ì´ ë§í•˜ê¸¸... ë„ˆí•œí…Œ í•  ë§ì´ ì¢€ ìˆëŒ€.",
            "ğŸ”® ì˜ˆì–¸ì˜ ìˆ˜ì • êµ¬ìŠ¬ì— ë¯¸ë˜ê°€ ë³´ì—¬. ì´ëŒ€ë¡œë©´ ë‚´ë…„ì—ë„ ë˜‘ê°™ì€ ì‹¤ìˆ˜ ë°˜ë³µí•  í…ë°?",
        ]

        closing_prophecies = [
            "ğŸ’« ì´ ë…ì„¤ë“¤ì„ ë¬´ì‹œí•˜ë©´ ë‚´ë…„ì—ë„ ë˜‘ê°™ì€ ì–˜ê¸° ë“¤ì„ ê±°ì•¼. í•˜ì§€ë§Œ í•˜ë‚˜ì”©ë§Œ ê³ ì³ë„ í›¨ì”¬ ë‚˜ì•„ì§ˆ ê±°ë¼ëŠ” ê²ƒë„ ë³´ì—¬. ì„ íƒì€ ë„¤ ëª«ì´ì•¼.",
            "ğŸ’« ë§ˆë…€ì˜ ì¡°ì–¸ì€ ì—¬ê¸°ê¹Œì§€. ë“£ë“  ë§ë“  ë„ˆ ë§˜ì´ì§€ë§Œ, 1ë…„ í›„ ë” ë‚˜ì€ ê°œë°œìê°€ ë˜ê³  ì‹¶ë‹¤ë©´... ë­, ì•Œì•„ì„œ í•´.",
            "ğŸ’« ìˆ˜ì • êµ¬ìŠ¬ì´ ë³´ì—¬ì£¼ëŠ” ë¯¸ë˜: ì´ê²ƒë“¤ë§Œ ê³ ì¹˜ë©´ ë‚´ë…„ì—” ê½¤ ê´œì°®ì€ ê°œë°œìê°€ ë  ìˆ˜ ìˆì–´. ì•ˆ ê³ ì¹˜ë©´? ê·¸ê±´ ë„¤ê°€ ë” ì˜ ì•Œê² ì§€.",
        ]

        return WitchCritique(
            opening_curse=random.choice(opening_curses),
            critiques=critiques,
            closing_prophecy=random.choice(closing_prophecies)
        )

    def _check_commit_message_quality(
        self,
        detailed_feedback: Optional[DetailedFeedbackSnapshot],
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check commit message quality and add critique if poor.

        Args:
            detailed_feedback: Optional detailed feedback snapshot
            critiques: List to append critique to if issues found
        """
        if not detailed_feedback or not detailed_feedback.commit_feedback:
            return

        commit_fb = detailed_feedback.commit_feedback
        if commit_fb.total_commits == 0:
            return

        poor_ratio = commit_fb.poor_messages / commit_fb.total_commits
        if poor_ratio > CRITIQUE_THRESHOLDS['poor_commit_ratio']:
            critiques.append(
                WitchCritiqueItem(
                    category="ì»¤ë°‹ ë©”ì‹œì§€",
                    severity="ğŸ”¥ ì¹˜ëª…ì ",
                    critique=f"ì»¤ë°‹ ë©”ì‹œì§€ì˜ {poor_ratio*100:.0f}%ê°€ í˜•í¸ì—†ì–´. 'ìˆ˜ì •', 'fix', 'update' ê°™ì€ ê²Œ ì „ë¶€ì•¼? 6ê°œì›” í›„ ë„ˆ ìì‹ ë„ ë­˜ ê³ ì³¤ëŠ”ì§€ ëª¨ë¥¼ í…ë°.",
                    evidence=f"{commit_fb.total_commits}ê°œ ì»¤ë°‹ ì¤‘ {commit_fb.poor_messages}ê°œê°€ ë¶ˆëŸ‰",
                    consequence="ë‚˜ì¤‘ì— ë²„ê·¸ ì°¾ëŠë¼ git log ë³´ë©´ì„œ ì‹œê°„ ë‚­ë¹„í•  ê±°ì•¼. íŒ€ì›ë“¤ë„ ë„¤ ë³€ê²½ì‚¬í•­ ì´í•´ ëª» í•´.",
                    remedy="ì»¤ë°‹ ë©”ì‹œì§€ì— 'ì™œ'ë¥¼ ë‹´ì•„. 'fix: ë¡œê·¸ì¸ ì‹œ í† í° ë§Œë£Œ ì²´í¬ ëˆ„ë½ ìˆ˜ì •' ì´ëŸ° ì‹ìœ¼ë¡œ."
                )
            )

    def _check_pr_size(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check PR size and add critique if too large.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        if not collection.pull_request_examples:
            return

        large_prs = [pr for pr in collection.pull_request_examples
                    if (pr.additions + pr.deletions) > CRITIQUE_THRESHOLDS['large_pr_lines']]

        if len(large_prs) > len(collection.pull_request_examples) * CRITIQUE_THRESHOLDS['large_pr_ratio']:
            avg_size = sum(pr.additions + pr.deletions for pr in collection.pull_request_examples) / len(collection.pull_request_examples)
            critiques.append(
                WitchCritiqueItem(
                    category="PR í¬ê¸°",
                    severity="âš¡ ì‹¬ê°",
                    critique=f"PR í•˜ë‚˜ì— í‰ê·  {avg_size:.0f}ì¤„? ë¦¬ë·°ì–´ë“¤ ê´´ë¡­íˆëŠ” ê²Œ ì·¨ë¯¸ì•¼? í° PRì€ ì•ˆ ì½íŒë‹¤ëŠ” ê±° ëª°ë¼?",
                    evidence=f"{len(large_prs)}ê°œ PRì´ {CRITIQUE_THRESHOLDS['large_pr_lines']}ì¤„ ì´ìƒ",
                    consequence="ë¦¬ë·° í’ˆì§ˆ ë–¨ì–´ì§€ê³ , ë²„ê·¸ ë†“ì¹˜ê³ , ë¨¸ì§€ ì¶©ëŒ ì§€ì˜¥ì— ë¹ ì§ˆ ê±°ì•¼.",
                    remedy=f"PRì€ {CRITIQUE_THRESHOLDS['recommended_pr_size']}ì¤„ ì´í•˜ë¡œ. í° ê¸°ëŠ¥ì€ ìª¼ê°œì„œ ì—¬ëŸ¬ PRë¡œ ë‚˜ëˆ . Feature flag ì¨."
                )
            )

    def _check_pr_title_quality(
        self,
        detailed_feedback: Optional[DetailedFeedbackSnapshot],
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check PR title quality and add critique if vague.

        Args:
            detailed_feedback: Optional detailed feedback snapshot
            critiques: List to append critique to if issues found
        """
        if not detailed_feedback or not detailed_feedback.pr_title_feedback:
            return

        pr_fb = detailed_feedback.pr_title_feedback
        if pr_fb.total_prs == 0:
            return

        vague_ratio = pr_fb.vague_titles / pr_fb.total_prs
        if vague_ratio > CRITIQUE_THRESHOLDS['vague_title_ratio']:
            critiques.append(
                WitchCritiqueItem(
                    category="PR ì œëª©",
                    severity="ğŸ’€ ìœ„í—˜",
                    critique=f"PR ì œëª© {vague_ratio*100:.0f}%ê°€ ë­” ë§ì¸ì§€ ëª¨ë¥´ê² ì–´. 'ê¸°ëŠ¥ ì¶”ê°€', 'ë²„ê·¸ ìˆ˜ì •'? ì–´ë–¤ ê¸°ëŠ¥? ì–´ë–¤ ë²„ê·¸?",
                    evidence=f"{pr_fb.total_prs}ê°œ PR ì¤‘ {pr_fb.vague_titles}ê°œê°€ ëª¨í˜¸í•¨",
                    consequence="ë¦´ë¦¬ìŠ¤ ë…¸íŠ¸ ì“¸ ë•Œ ìš¸ê³ , ë‚˜ì¤‘ì— ì°¾ì„ ë•Œ ì‚½ì§ˆí•˜ê³ .",
                    remedy="'feat: ì‚¬ìš©ì í”„ë¡œí•„ì— ì•„ë°”íƒ€ ì—…ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€' ì´ëŸ° ì‹ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ."
                )
            )

    def _check_review_quality(
        self,
        collection: CollectionResult,
        detailed_feedback: Optional[DetailedFeedbackSnapshot],
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check review quality and frequency, add critique if insufficient.

        Args:
            collection: Collection of repository data
            detailed_feedback: Optional detailed feedback snapshot
            critiques: List to append critique to if issues found
        """
        if detailed_feedback and detailed_feedback.review_tone_feedback:
            review_fb = detailed_feedback.review_tone_feedback
            if review_fb.total_reviews > 0:
                # Check if reviews are too short/neutral (may indicate low quality)
                low_quality_ratio = review_fb.neutral_reviews / review_fb.total_reviews
                if low_quality_ratio > CRITIQUE_THRESHOLDS['neutral_review_ratio']:
                    critiques.append(
                        WitchCritiqueItem(
                            category="ì½”ë“œ ë¦¬ë·°",
                            severity="ğŸ•·ï¸ ê²½ê³ ",
                            critique=f"ë¦¬ë·°ì˜ {low_quality_ratio*100:.0f}%ê°€ ê·¸ëƒ¥ 'LGTM' ìˆ˜ì¤€ì´ì•¼. ì§„ì§œ ì½”ë“œ ì½ê¸´ í•œ ê±°ì•¼?",
                            evidence=f"{review_fb.total_reviews}ê°œ ë¦¬ë·° ì¤‘ {review_fb.neutral_reviews}ê°œê°€ í˜•ì‹ì ",
                            consequence="íŒ€ ì½”ë“œ í’ˆì§ˆ ë–¨ì–´ì§€ê³ , ë²„ê·¸ í”„ë¡œë•ì…˜ì—ì„œ ë°œê²¬ë˜ê³ .",
                            remedy="êµ¬ì²´ì ì¸ í”¼ë“œë°± ì¤˜. 'ì´ í•¨ìˆ˜ ë³µì¡ë„ ë†’ì€ë° í…ŒìŠ¤íŠ¸ ì¶”ê°€í•˜ë©´ ì–´ë•Œ?' ì´ëŸ° ì‹ìœ¼ë¡œ."
                        )
                    )
        elif collection.reviews < collection.pull_requests * CRITIQUE_THRESHOLDS['review_pr_ratio']:
            # Not enough reviews compared to PRs
            critiques.append(
                WitchCritiqueItem(
                    category="ì½”ë“œ ë¦¬ë·° ì°¸ì—¬",
                    severity="âš¡ ì‹¬ê°",
                    critique=f"PRì€ {collection.pull_requests}ê°œì¸ë° ë¦¬ë·°ëŠ” {collection.reviews}ê°œ? ë‚¨ì˜ ì½”ë“œëŠ” ì•ˆ ë´?",
                    evidence=f"PR ëŒ€ë¹„ ë¦¬ë·° ë¹„ìœ¨: {(collection.reviews/max(collection.pull_requests,1))*100:.0f}%",
                    consequence="íŒ€ì—ì„œ ì™¸í†¨ì´ ë˜ê³ , ë„¤ PRë„ ë¦¬ë·° ì•ˆ ë°›ê²Œ ë  ê±°ì•¼.",
                    remedy="í•˜ë£¨ì— ìµœì†Œ 2ê°œ PRì€ ë¦¬ë·°í•´. ë‚¨ì˜ ì½”ë“œ ë³´ëŠ” ê²Œ ìµœê³ ì˜ í•™ìŠµì´ì•¼."
                )
            )

    def _check_activity_consistency(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check activity consistency and add critique if too sporadic.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        if collection.commits == 0 or collection.months == 0:
            return

        commits_per_month = collection.commits / collection.months
        if commits_per_month < CRITIQUE_THRESHOLDS['min_commits_per_month']:
            critiques.append(
                WitchCritiqueItem(
                    category="í™œë™ ì¼ê´€ì„±",
                    severity="ğŸ•·ï¸ ê²½ê³ ",
                    critique=f"ì›”í‰ê·  {commits_per_month:.1f}ê°œ ì»¤ë°‹? ë©°ì¹  ëª°ì•„ì¹˜ê³  ì‰¬ëŠ” ìŠ¤íƒ€ì¼ì´ì§€? ê°œë°œì€ ë§ˆë¼í†¤ì´ì•¼, ë‹¨ê±°ë¦¬ ë‹¬ë¦¬ê¸°ê°€ ì•„ë‹ˆë¼.",
                    evidence=f"{collection.months}ê°œì›”ê°„ {collection.commits}ê°œ ì»¤ë°‹",
                    consequence="ì½”ë“œ í’ˆì§ˆ ë“¤ì­‰ë‚ ì­‰í•˜ê³ , íŒ€ í˜‘ì—… íƒ€ì´ë° ì•ˆ ë§ê³ .",
                    remedy="ë§¤ì¼ ì¡°ê¸ˆì”© ê¾¸ì¤€íˆ. ì‘ì€ ì»¤ë°‹ì´ë¼ë„ ë§¤ì¼ í•˜ëŠ” ê²Œ ì›”ë§ì— ëª°ì•„ì¹˜ëŠ” ê²ƒë³´ë‹¤ ë‚«ë‹¤."
                )
            )

    def _check_documentation_culture(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check documentation practices and add critique if insufficient.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        if not collection.pull_request_examples:
            return

        # Count documentation-related PRs
        doc_keywords = ['doc', 'readme', 'ë¬¸ì„œ', 'documentation', 'guide']
        doc_prs = [pr for pr in collection.pull_request_examples
                   if any(kw in pr.title.lower() for kw in doc_keywords)]

        doc_ratio = len(doc_prs) / len(collection.pull_request_examples)
        if doc_ratio < CRITIQUE_THRESHOLDS['min_doc_pr_ratio']:
            critiques.append(
                WitchCritiqueItem(
                    category="ë¬¸ì„œí™”",
                    severity="ğŸ•·ï¸ ê²½ê³ ",
                    critique=f"ë¬¸ì„œ ê´€ë ¨ PRì´ ì „ì²´ì˜ {doc_ratio*100:.0f}%ë°–ì— ì•ˆ ë¼? 6ê°œì›” í›„ ë„¤ ì½”ë“œ ì´í•´ ëª» í•˜ëŠ” ê±´ ë„ˆ ìì‹ ì´ì•¼.",
                    evidence=f"{len(collection.pull_request_examples)}ê°œ PR ì¤‘ {len(doc_prs)}ê°œë§Œ ë¬¸ì„œ ê´€ë ¨",
                    consequence="ì‹ ê·œ íŒ€ì› ì˜¨ë³´ë”© ì§€ì˜¥, API ì‚¬ìš©ë²• ë¬¼ì–´ë³´ëŠ” ìŠ¬ë™ ë©”ì‹œì§€ í­íƒ„, ë ˆê±°ì‹œ ì½”ë“œí™” ê°€ì†.",
                    remedy="README ì—…ë°ì´íŠ¸, API ë¬¸ì„œí™”, ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì¶”ê°€. ì½”ë“œë§Œí¼ ë¬¸ì„œë„ ì¤‘ìš”í•´."
                )
            )

    def _check_test_coverage(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check test-related activity and add critique if insufficient.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        if not collection.pull_request_examples:
            return

        # Count test-related PRs
        test_keywords = ['test', 'í…ŒìŠ¤íŠ¸', 'spec', 'unittest', 'integration']
        test_prs = [pr for pr in collection.pull_request_examples
                    if any(kw in pr.title.lower() for kw in test_keywords)]

        test_ratio = len(test_prs) / len(collection.pull_request_examples)
        if test_ratio < CRITIQUE_THRESHOLDS['min_test_pr_ratio']:
            critiques.append(
                WitchCritiqueItem(
                    category="í…ŒìŠ¤íŠ¸",
                    severity="âš¡ ì‹¬ê°",
                    critique=f"í…ŒìŠ¤íŠ¸ ê´€ë ¨ PRì´ {test_ratio*100:.0f}%? í”„ë¡œë•ì…˜ì´ ë„¤ í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ì•¼? ëŒ€ë‹´í•œë°?",
                    evidence=f"{len(collection.pull_request_examples)}ê°œ PR ì¤‘ {len(test_prs)}ê°œë§Œ í…ŒìŠ¤íŠ¸ ê´€ë ¨",
                    consequence="í”„ë¡œë•ì…˜ ë²„ê·¸, ìƒˆë²½ 3ì‹œ ê¸´ê¸‰ ë°°í¬, ì‚¬ìš©ì ì´íƒˆ, íŒ€ ì‹ ë¢°ë„ ì¶”ë½.",
                    remedy="í•µì‹¬ ë¡œì§ í…ŒìŠ¤íŠ¸ ì‘ì„±, CIì— í…ŒìŠ¤íŠ¸ í•„ìˆ˜í™”, ì»¤ë²„ë¦¬ì§€ 60% ëª©í‘œ. 'ëŒì•„ê°„ë‹¤'ë¡œ ë§Œì¡±í•˜ì§€ ë§ˆ."
                )
            )

    def _check_branch_management(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check branch management practices and add critique if messy.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        if not collection.pull_request_examples or collection.pull_requests == 0:
            return

        # Calculate average commits per PR
        avg_commits_per_pr = collection.commits / collection.pull_requests
        if avg_commits_per_pr > CRITIQUE_THRESHOLDS['max_commits_per_pr']:
            critiques.append(
                WitchCritiqueItem(
                    category="ë¸Œëœì¹˜ ê´€ë¦¬",
                    severity="ğŸ•·ï¸ ê²½ê³ ",
                    critique=f"PRë‹¹ í‰ê·  {avg_commits_per_pr:.1f}ê°œ ì»¤ë°‹? ë¸Œëœì¹˜ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë²Œì–´ì§€ëŠ” ê±°ì•¼? ì •ë¦¬ ì¢€ í•´.",
                    evidence=f"{collection.commits}ê°œ ì»¤ë°‹ / {collection.pull_requests}ê°œ PR",
                    consequence="ë¦¬ë·°ì–´ í˜¼ë€, ë¨¸ì§€ ì¶©ëŒ ì§€ì˜¥, Git íˆìŠ¤í† ë¦¬ ë‚œì¥íŒ.",
                    remedy="ê¸°ëŠ¥ë³„ë¡œ ë¸Œëœì¹˜ ë¶„ë¦¬, ì‘ì€ ë‹¨ìœ„ë¡œ ìì£¼ PR, ë¦¬ë² ì´ìŠ¤ë¡œ ì»¤ë°‹ ì •ë¦¬. ê¹”ë”í•œ íˆìŠ¤í† ë¦¬ê°€ í”„ë¡œì•¼."
                )
            )

    def _check_issue_tracking(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check issue tracking practices and add critique if insufficient.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        if collection.commits == 0 and collection.pull_requests == 0:
            return

        total_activity = collection.commits + collection.pull_requests + collection.reviews
        if total_activity == 0:
            return

        issue_ratio = collection.issues / total_activity
        if issue_ratio < CRITIQUE_THRESHOLDS['min_issue_ratio']:
            critiques.append(
                WitchCritiqueItem(
                    category="ì´ìŠˆ ì¶”ì ",
                    severity="ğŸ•·ï¸ ê²½ê³ ",
                    critique=f"ì „ì²´ í™œë™ì˜ {issue_ratio*100:.0f}%ë§Œ ì´ìŠˆ? ë²„ê·¸ëŠ” ì—†ì–´? ì•„ë‹ˆë©´ ê·¸ëƒ¥ ì¶”ì  ì•ˆ í•˜ëŠ” ê±°ì•¼?",
                    evidence=f"ì´ {total_activity}ê±´ í™œë™ ì¤‘ {collection.issues}ê±´ë§Œ ì´ìŠˆ",
                    consequence="ë²„ê·¸ ì¬ë°œ, ìš”êµ¬ì‚¬í•­ ì¶”ì  ë¶ˆê°€, í”„ë¡œì íŠ¸ ê´€ë¦¬ ì‹¤íŒ¨, ìš°ì„ ìˆœìœ„ í˜¼ë€.",
                    remedy="ë²„ê·¸ ë°œê²¬í•˜ë©´ ì´ìŠˆ ìƒì„±, ê¸°ëŠ¥ ìš”ì²­ë„ ì´ìŠˆë¡œ ê´€ë¦¬, ë¼ë²¨ë§ ì²´ê³„í™”. ì²´ê³„ì ì¸ ì¶”ì ì´ í”„ë¡œì íŠ¸ ì„±ê³µì˜ ì—´ì‡ ì•¼."
                )
            )

    def _check_collaboration_diversity(
        self,
        collection: CollectionResult,
        critiques: List[WitchCritiqueItem]
    ) -> None:
        """Check collaboration diversity and add critique if too isolated.

        Args:
            collection: Collection of repository data
            critiques: List to append critique to if issues found
        """
        # This check would ideally use collaboration data, but we can infer from PR/review ratio
        if collection.pull_requests == 0:
            return

        # If someone has many PRs but very few reviews, they might be working in isolation
        review_to_pr_ratio = collection.reviews / collection.pull_requests if collection.pull_requests > 0 else 0

        if review_to_pr_ratio < 0.3 and collection.pull_requests > 5:
            critiques.append(
                WitchCritiqueItem(
                    category="í˜‘ì—… ë‹¤ì–‘ì„±",
                    severity="ğŸ•·ï¸ ê²½ê³ ",
                    critique=f"PRì€ {collection.pull_requests}ê°œì¸ë° ë¦¬ë·°ëŠ” {collection.reviews}ê°œ? í˜¼ì ì„¬ì—ì„œ ì½”ë”©í•˜ëŠ” ê¸°ë¶„ì´ì•¼?",
                    evidence=f"PR ëŒ€ë¹„ ë¦¬ë·° ë¹„ìœ¨: {review_to_pr_ratio*100:.0f}%",
                    consequence="íŒ€ ë‚´ ì§€ì‹ ì‚¬ì¼ë¡œ, ì½”ë“œ í’ˆì§ˆ ì €í•˜, ë²„ìŠ¤ íŒ©í„° 1, ì™¸í†¨ì´ ê°œë°œì.",
                    remedy="ë‹¤ì–‘í•œ íŒ€ì›ê³¼ í˜‘ì—…, ì •ê¸°ì  ì½”ë“œ ë¦¬ë·° ì°¸ì—¬, í˜ì–´ í”„ë¡œê·¸ë˜ë° ì‹œë„. í˜¼ì ì˜í•´ë´¤ì í•œê³„ ìˆì–´."
                )
            )

    def _get_random_general_critique(self, collection: CollectionResult) -> WitchCritiqueItem:
        """Get a random general critique for developers with no specific issues.

        Args:
            collection: Collection of repository data for evidence text

        Returns:
            A randomly selected general improvement critique
        """
        import random

        general_critiques = [
            WitchCritiqueItem(
                category="ê°œë°œì ì„±ì¥",
                severity="ğŸ’« ì¡°ì–¸",
                critique="ê²‰ìœ¼ë¡œëŠ” ê´œì°®ì•„ ë³´ì´ì§€ë§Œ, ì•ˆì£¼í•˜ë©´ í‡´ë³´í•˜ëŠ” ë²•ì´ì•¼. ì§€ê¸ˆì´ ë”± ë‹¤ìŒ ë ˆë²¨ë¡œ ì˜¬ë¼ê°ˆ ë•Œì•¼.",
                evidence=f"ì´ {collection.commits}ê°œ ì»¤ë°‹, {collection.pull_requests}ê°œ PR ë¶„ì„ ì™„ë£Œ",
                consequence="í˜„ìƒ ìœ ì§€ëŠ” ê³§ ë’¤ì²˜ì§€ëŠ” ê±°ì•¼. ê¸°ìˆ ì€ ë§¤ì¼ ë°œì „í•˜ëŠ”ë° ë„ˆë§Œ ê·¸ ìë¦¬ë©´?",
                remedy="ìƒˆë¡œìš´ ê¸°ìˆ  í•˜ë‚˜ ë°°ì›Œë´. ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬í•˜ê±°ë‚˜, ë” ì–´ë ¤ìš´ ë¬¸ì œì— ë„ì „í•´ë´."
            ),
            WitchCritiqueItem(
                category="ì½”ë“œ í’ˆì§ˆ",
                severity="ğŸ’« ì¡°ì–¸",
                critique="ì½”ë“œëŠ” ì¼ë‹¨ ëŒì•„ê°€ëŠ”ë°... ê·¸ëƒ¥ 'ëŒì•„ê°„ë‹¤'ë¡œ ë§Œì¡±í•  ê±°ì•¼? ì•„ë‹ˆë©´ 'ì•„ë¦„ë‹µê²Œ ëŒì•„ê°„ë‹¤'ë¥¼ ëª©í‘œë¡œ í•  ê±°ì•¼?",
                evidence="ì»¤ë°‹ íˆìŠ¤í† ë¦¬ ì „ì²´ ë¶„ì„ ì™„ë£Œ",
                consequence="ë™ì‘í•˜ëŠ” ì½”ë“œì™€ í›Œë¥­í•œ ì½”ë“œì˜ ì°¨ì´ë¥¼ ëª¨ë¥´ë©´, ì˜ì›íˆ ì‹œë‹ˆì–´ ê°œë°œì ëª» ë¼.",
                remedy="ë¦¬íŒ©í† ë§ì— ì‹œê°„ íˆ¬ìí•´. í´ë¦° ì½”ë“œ ì›ì¹™ ê³µë¶€í•˜ê³ , ì½”ë“œ ë¦¬ë·°ì—ì„œ ë” ë§ì´ ë°°ì›Œ."
            ),
            WitchCritiqueItem(
                category="í˜‘ì—… ëŠ¥ë ¥",
                severity="ğŸ’« ì¡°ì–¸",
                critique="í˜¼ìì„œëŠ” ì˜í•˜ëŠ”ë°, íŒ€ì›Œí¬ëŠ” ì–´ë•Œ? ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ë„ ê¸°ìˆ ì´ì•¼. ì½”ë”©ë§Œ ì˜í•œë‹¤ê³  ë‹¤ê°€ ì•„ë‹ˆë¼ê³ .",
                evidence=f"PR {collection.pull_requests}ê°œ, ë¦¬ë·° {collection.reviews}ê°œ í™œë™ í™•ì¸",
                consequence="í˜‘ì—… ëª» í•˜ëŠ” ê°œë°œìëŠ” í˜¼ì í•  ìˆ˜ ìˆëŠ” ê²ƒë§Œ í•  ìˆ˜ ìˆì–´. í° í”„ë¡œì íŠ¸ëŠ” ë¬´ë¦¬.",
                remedy="PR ì„¤ëª… ë” ìì„¸íˆ ì¨. ë¦¬ë·° ëŒ“ê¸€ì— ì´ìœ ì™€ ëŒ€ì•ˆ ì œì‹œí•´. íŒ€ì›ë“¤ê³¼ ë” ì†Œí†µí•´."
            ),
            WitchCritiqueItem(
                category="í•™ìŠµ íƒœë„",
                severity="ğŸ’« ì¡°ì–¸",
                critique="ìµìˆ™í•œ ê²ƒë§Œ ë°˜ë³µí•˜ê³  ìˆì§€ ì•Šì•„? í¸ì•ˆí•¨(comfort zone)ì— ë¨¸ë¬´ë¥´ë©´ ì„±ì¥ ì—†ì–´.",
                evidence="í™œë™ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ",
                consequence="5ë…„ì°¨ì¸ë° 1ë…„ì°¨ ì‹¤ë ¥ë§Œ ìˆëŠ” ê°œë°œì ë˜ê¸° ì‹«ìœ¼ë©´ ë³€í™” í•„ìš”í•´.",
                remedy="ë§¤ë‹¬ ìƒˆë¡œìš´ ê²ƒ í•˜ë‚˜ì”© ì‹œë„í•´. ë‚¯ì„  ë¼ì´ë¸ŒëŸ¬ë¦¬, ë‹¤ë¥¸ íŒ¨ëŸ¬ë‹¤ì„, ìƒˆë¡œìš´ ë„êµ¬."
            ),
            WitchCritiqueItem(
                category="ë¬¸ì„œí™”",
                severity="ğŸ’« ì¡°ì–¸",
                critique="ì½”ë“œëŠ” ì“°ëŠ”ë° ë¬¸ì„œëŠ”? 6ê°œì›” í›„ ë„¤ ì½”ë“œ ë‹¤ì‹œ ë³¼ ë•Œ ì£¼ì„ ì—†ì–´ì„œ í›„íšŒí•˜ëŠ” ê±´ ë„ˆì•¼.",
                evidence="ì»¤ë°‹ ë° PR íŒ¨í„´ ë¶„ì„",
                consequence="ë¬¸ì„œ ì—†ëŠ” ì½”ë“œëŠ” ë ˆê±°ì‹œê°€ ë˜ëŠ” ìˆœê°„ ì•„ë¬´ë„ ëª» ê±´ë“œë ¤. ë„ˆë„ ëª» ê±´ë“œë¦¬ê²Œ ë¼.",
                remedy="ë³µì¡í•œ ë¡œì§ì—ëŠ” ì£¼ì„ ë‹¬ì•„. README ì—…ë°ì´íŠ¸í•´. APIëŠ” ë¬¸ì„œí™”í•´."
            ),
            WitchCritiqueItem(
                category="í…ŒìŠ¤íŠ¸ ë¬¸í™”",
                severity="ğŸ’« ì¡°ì–¸",
                critique="í…ŒìŠ¤íŠ¸ ì—†ì´ ì½”ë“œ ì§œê³  ìˆëŠ” ê±´ ì•„ë‹ˆê² ì§€? 'ëŒì•„ê°€ë‹ˆê¹Œ ëì§€'ëŠ” ì´ˆë³´ ë§ˆì¸ë“œì•¼.",
                evidence="ì „ì²´ ê°œë°œ í™œë™ ê²€í† ",
                consequence="í…ŒìŠ¤íŠ¸ ì—†ëŠ” ë¦¬íŒ©í† ë§ì€ ìì‚´í–‰ìœ„. ì–¸ì  ê°€ ë°°í¬í•˜ê³  ë°¤ìƒˆ ë¡¤ë°±í•˜ëŠ” ë‚  ì˜¬ ê±°ì•¼.",
                remedy="TDDëŠ” ì•„ë‹ˆì–´ë„, í•µì‹¬ ë¡œì§ì€ í…ŒìŠ¤íŠ¸ ì‘ì„±í•´. Coverage 60% ì´ìƒ ëª©í‘œë¡œ."
            ),
        ]

        return random.choice(general_critiques)

    def _build_stats(self, collection: CollectionResult, velocity_score: float) -> Dict[str, Dict[str, float]]:
        return {
            "commits": {
                "total": float(collection.commits),
                "per_month": velocity_score,
            },
            "pull_requests": {
                "total": float(collection.pull_requests),
            },
            "reviews": {
                "total": float(collection.reviews),
            },
            "issues": {
                "total": float(collection.issues),
            },
        }

    def _build_evidence(self, collection: CollectionResult) -> Dict[str, List[str]]:
        repo_root = f"{self.web_base_url.rstrip('/')}/{collection.repo}"
        return {
            "commits": [
                f"{repo_root}/commits",
            ],
            "pull_requests": [
                f"{repo_root}/pulls",
            ],
        }

    def _build_commit_feedback(self, analysis: Dict) -> CommitMessageFeedback:
        """Build commit message feedback from analysis."""
        return CommitMessageFeedback(
            total_commits=analysis.get("good_messages", 0) + analysis.get("poor_messages", 0),
            good_messages=analysis.get("good_messages", 0),
            poor_messages=analysis.get("poor_messages", 0),
            suggestions=analysis.get("suggestions", []),
            examples_good=analysis.get("examples_good", []),
            examples_poor=analysis.get("examples_poor", []),
        )

    def _build_pr_title_feedback(self, analysis: Dict) -> PRTitleFeedback:
        """Build PR title feedback from analysis."""
        return PRTitleFeedback(
            total_prs=analysis.get("clear_titles", 0) + analysis.get("vague_titles", 0),
            clear_titles=analysis.get("clear_titles", 0),
            vague_titles=analysis.get("vague_titles", 0),
            suggestions=analysis.get("suggestions", []),
            examples_good=analysis.get("examples_good", []),
            examples_poor=analysis.get("examples_poor", []),
        )

    def _build_review_tone_feedback(self, analysis: Dict) -> ReviewToneFeedback:
        """Build review tone feedback from analysis."""
        return ReviewToneFeedback(
            total_reviews=analysis.get("constructive_reviews", 0)
            + analysis.get("harsh_reviews", 0)
            + analysis.get("neutral_reviews", 0),
            constructive_reviews=analysis.get("constructive_reviews", 0),
            harsh_reviews=analysis.get("harsh_reviews", 0),
            neutral_reviews=analysis.get("neutral_reviews", 0),
            suggestions=analysis.get("suggestions", []),
            examples_good=analysis.get("examples_good", []),
            examples_improve=analysis.get("examples_improve", []),
        )

    def _build_issue_feedback(self, analysis: Dict) -> IssueFeedback:
        """Build issue feedback from analysis."""
        return IssueFeedback(
            total_issues=analysis.get("well_described", 0) + analysis.get("poorly_described", 0),
            well_described=analysis.get("well_described", 0),
            poorly_described=analysis.get("poorly_described", 0),
            suggestions=analysis.get("suggestions", []),
            examples_good=analysis.get("examples_good", []),
            examples_poor=analysis.get("examples_poor", []),
        )

    def _build_personal_development_analysis(self, analysis: Dict) -> PersonalDevelopmentAnalysis:
        """Build personal development analysis from LLM response."""
        # Parse strengths
        strengths = []
        for strength_data in analysis.get("strengths", []):
            if not isinstance(strength_data, dict):
                continue
            strengths.append(
                StrengthPoint(
                    category=strength_data.get("category", ""),
                    description=strength_data.get("description", ""),
                    evidence=strength_data.get("evidence", []),
                    impact=strength_data.get("impact", "medium"),
                )
            )

        # Parse improvement areas
        improvement_areas = []
        for improvement_data in analysis.get("improvement_areas", []):
            if not isinstance(improvement_data, dict):
                continue
            improvement_areas.append(
                ImprovementArea(
                    category=improvement_data.get("category", ""),
                    description=improvement_data.get("description", ""),
                    evidence=improvement_data.get("evidence", []),
                    suggestions=improvement_data.get("suggestions", []),
                    priority=improvement_data.get("priority", "medium"),
                )
            )

        # Parse growth indicators
        growth_indicators = []
        for growth_data in analysis.get("growth_indicators", []):
            if not isinstance(growth_data, dict):
                continue
            growth_indicators.append(
                GrowthIndicator(
                    aspect=growth_data.get("aspect", ""),
                    description=growth_data.get("description", ""),
                    before_examples=growth_data.get("before_examples", []),
                    after_examples=growth_data.get("after_examples", []),
                    progress_summary=growth_data.get("progress_summary", ""),
                )
            )

        return PersonalDevelopmentAnalysis(
            strengths=strengths,
            improvement_areas=improvement_areas,
            growth_indicators=growth_indicators,
            overall_assessment=analysis.get("overall_assessment", ""),
            key_achievements=analysis.get("key_achievements", []),
            next_focus_areas=analysis.get("next_focus_areas", []),
        )

    def build_detailed_feedback(
        self,
        commit_analysis: Optional[Dict] = None,
        pr_title_analysis: Optional[Dict] = None,
        review_tone_analysis: Optional[Dict] = None,
        issue_analysis: Optional[Dict] = None,
        personal_development_analysis: Optional[Dict] = None,
    ) -> DetailedFeedbackSnapshot:
        """Build detailed feedback snapshot from LLM analysis results."""

        return DetailedFeedbackSnapshot(
            commit_feedback=self._build_commit_feedback(commit_analysis) if commit_analysis else None,
            pr_title_feedback=self._build_pr_title_feedback(pr_title_analysis) if pr_title_analysis else None,
            review_tone_feedback=self._build_review_tone_feedback(review_tone_analysis) if review_tone_analysis else None,
            issue_feedback=self._build_issue_feedback(issue_analysis) if issue_analysis else None,
            personal_development=self._build_personal_development_analysis(personal_development_analysis) if personal_development_analysis else None,
        )

    def _build_monthly_trends(
        self, monthly_trends_data: Optional[List[Dict]]
    ) -> List[MonthlyTrend]:
        """Build monthly trend objects from raw data."""
        if not monthly_trends_data:
            return []

        trends = []
        for data in monthly_trends_data:
            trends.append(
                MonthlyTrend(
                    month=data.get("month", ""),
                    commits=data.get("commits", 0),
                    pull_requests=data.get("pull_requests", 0),
                    reviews=data.get("reviews", 0),
                    issues=data.get("issues", 0),
                )
            )
        return trends

    def _calculate_trend_direction(self, monthly_activities: List[tuple]) -> str:
        """Calculate trend direction from monthly activities.

        Algorithm:
        1. Requires minimum number of months (from TREND_THRESHOLDS)
        2. Splits activity data into two halves (early vs recent)
        3. Compares average activity between halves
        4. Returns 'increasing' if recent > early * multiplier
        5. Returns 'decreasing' if recent < early * multiplier
        6. Returns 'stable' otherwise

        Args:
            monthly_activities: List of (month, activity_count) tuples

        Returns:
            One of: "increasing", "decreasing", or "stable"
        """
        if len(monthly_activities) < TREND_THRESHOLDS['minimum_months_for_trend']:
            return "stable"

        recent_half = monthly_activities[len(monthly_activities)//2:]
        early_half = monthly_activities[:len(monthly_activities)//2]

        recent_avg = sum(act for _, act in recent_half) / len(recent_half) if recent_half else 0
        early_avg = sum(act for _, act in early_half) / len(early_half) if early_half else 0

        if recent_avg > early_avg * TREND_THRESHOLDS['increasing_multiplier']:
            return "increasing"
        elif recent_avg < early_avg * TREND_THRESHOLDS['decreasing_multiplier']:
            return "decreasing"
        else:
            return "stable"

    def _calculate_consistency_score(self, monthly_activities: List[tuple]) -> float:
        """Calculate consistency score from monthly activities.

        Uses coefficient of variation (CV) to measure consistency:
        - CV = standard_deviation / mean
        - Lower CV indicates more consistent activity
        - Returns score from 0 (highly variable) to 1 (perfectly consistent)
        """
        activities = [act for _, act in monthly_activities if act > 0]
        if not activities or len(activities) < 2:
            return 0.0

        mean_activity = sum(activities) / len(activities)
        variance = sum((act - mean_activity) ** 2 for act in activities) / len(activities)
        std_dev = math.sqrt(variance)

        # Coefficient of variation (lower is more consistent)
        cv = std_dev / mean_activity if mean_activity > 0 else 1.0
        # Convert to 0-1 score (1 = perfect consistency, 0 = highly variable)
        return max(0.0, 1.0 - min(cv, 1.0))

    def _generate_trend_insights(
        self,
        monthly_trends: List[MonthlyTrend],
        monthly_activities: List[tuple],
        peak_month: Optional[str],
        quiet_month: Optional[str],
        trend_direction: str,
        consistency_score: float,
    ) -> List[str]:
        """Generate human-readable insights from trend data."""
        insights = []

        if peak_month:
            peak_activity = next((act for month, act in monthly_activities if month == peak_month), 0)
            insights.append(
                f"{peak_month}ì— ê°€ì¥ í™œë°œí–ˆìŠµë‹ˆë‹¤ (ì´ {peak_activity}ê±´ì˜ í™œë™)"
            )

        if quiet_month and quiet_month != peak_month:
            quiet_activity = next((act for month, act in monthly_activities if month == quiet_month), 0)
            insights.append(
                f"{quiet_month}ì—ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì¡°ìš©í–ˆìŠµë‹ˆë‹¤ (ì´ {quiet_activity}ê±´ì˜ í™œë™)"
            )

        if trend_direction == "increasing":
            insights.append(
                "ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ í™œë™ëŸ‰ì´ ì¦ê°€í•˜ëŠ” ì„±ì¥ ì¶”ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤"
            )
        elif trend_direction == "decreasing":
            insights.append(
                "ìµœê·¼ í™œë™ëŸ‰ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë™ê¸° ë¶€ì—¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            )
        else:
            insights.append(
                "ê¾¸ì¤€í•œ í™œë™ ìˆ˜ì¤€ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤"
            )

        if consistency_score > CONSISTENCY_THRESHOLDS['very_consistent']:
            insights.append(
                f"ë§¤ìš° ì¼ê´€ëœ í™œë™ íŒ¨í„´ì„ ë³´ì˜€ìŠµë‹ˆë‹¤ (ì¼ê´€ì„± ì ìˆ˜: {consistency_score:.1%})"
            )
        elif consistency_score < CONSISTENCY_THRESHOLDS['inconsistent']:
            insights.append(
                f"í™œë™ëŸ‰ì˜ ë³€ë™ì´ í° í¸ì…ë‹ˆë‹¤ (ì¼ê´€ì„± ì ìˆ˜: {consistency_score:.1%}). "
                "ë” ê· í˜•ì¡íŒ ê¸°ì—¬ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”"
            )

        # Analyze specific activity types
        commits_trend = [trend.commits for trend in monthly_trends]
        prs_trend = [trend.pull_requests for trend in monthly_trends]

        if commits_trend and max(commits_trend) > 0:
            peak_commit_month = monthly_trends[commits_trend.index(max(commits_trend))].month
            insights.append(
                f"ì»¤ë°‹ í™œë™ì€ {peak_commit_month}ì— ì •ì ì„ ì°ì—ˆìŠµë‹ˆë‹¤ ({max(commits_trend)}íšŒ)"
            )

        if prs_trend and max(prs_trend) > 0:
            peak_pr_month = monthly_trends[prs_trend.index(max(prs_trend))].month
            if max(prs_trend) >= ACTIVITY_THRESHOLDS['moderate_prs']:
                insights.append(
                    f"PR í™œë™ì€ {peak_pr_month}ì— ê°€ì¥ ì™•ì„±í–ˆìŠµë‹ˆë‹¤ ({max(prs_trend)}ê°œ)"
                )

        return insights

    def _build_monthly_insights(
        self, monthly_trends: List[MonthlyTrend]
    ) -> Optional[MonthlyTrendInsights]:
        """Analyze monthly trends and generate insights."""
        if not monthly_trends or len(monthly_trends) < 2:
            return None

        # Calculate total activity per month
        monthly_activities = [
            (trend.month, trend.commits + trend.pull_requests + trend.reviews + trend.issues)
            for trend in monthly_trends
        ]

        # Find peak and quiet months
        peak_month_data = max(monthly_activities, key=lambda x: x[1])
        peak_month = peak_month_data[0] if peak_month_data[1] > 0 else None

        non_zero_activities = [(month, activity) for month, activity in monthly_activities if activity > 0]
        quiet_month = None
        if non_zero_activities:
            quiet_month_data = min(non_zero_activities, key=lambda x: x[1])
            quiet_month = quiet_month_data[0]

        # Calculate active months
        total_active_months = sum(1 for _, activity in monthly_activities if activity > 0)

        # Calculate metrics
        trend_direction = self._calculate_trend_direction(monthly_activities)
        consistency_score = self._calculate_consistency_score(monthly_activities)

        # Generate insights
        insights = self._generate_trend_insights(
            monthly_trends,
            monthly_activities,
            peak_month,
            quiet_month,
            trend_direction,
            consistency_score,
        )

        return MonthlyTrendInsights(
            peak_month=peak_month,
            quiet_month=quiet_month,
            trend_direction=trend_direction,
            total_active_months=total_active_months,
            consistency_score=consistency_score,
            insights=insights,
        )

    def _build_tech_stack_analysis(
        self, tech_stack_data: Optional[Dict[str, int]]
    ) -> Optional[TechStackAnalysis]:
        """Analyze technology stack from file changes."""
        if not tech_stack_data:
            return None

        # Calculate top languages
        sorted_languages = sorted(
            tech_stack_data.items(), key=lambda x: x[1], reverse=True
        )
        top_languages = [lang for lang, _ in sorted_languages[:DISPLAY_LIMITS['top_languages']]]

        # Calculate diversity score (Shannon entropy normalized)
        total_files = sum(tech_stack_data.values())
        if total_files == 0:
            diversity_score = 0.0
        else:
            import math
            entropy = 0.0
            for count in tech_stack_data.values():
                if count > 0:
                    p = count / total_files
                    entropy -= p * math.log2(p)
            # Normalize to 0-1 range (max entropy is log2(n))
            max_entropy = math.log2(len(tech_stack_data)) if len(tech_stack_data) > 1 else 1
            diversity_score = entropy / max_entropy if max_entropy > 0 else 0.0

        return TechStackAnalysis(
            languages=tech_stack_data,
            top_languages=top_languages,
            diversity_score=diversity_score,
        )

    def _build_collaboration_network(
        self, collaboration_data: Optional[Dict[str, Any]]
    ) -> Optional[CollaborationNetwork]:
        """Build collaboration network from reviewer data."""
        if not collaboration_data:
            return None

        pr_reviewers = collaboration_data.get("pr_reviewers", {})
        sorted_reviewers = sorted(
            pr_reviewers.items(), key=lambda x: x[1], reverse=True
        )
        top_reviewers = [reviewer for reviewer, _ in sorted_reviewers[:DISPLAY_LIMITS['top_reviewers']]]

        return CollaborationNetwork(
            pr_reviewers=pr_reviewers,
            top_reviewers=top_reviewers,
            review_received_count=collaboration_data.get("review_received_count", 0),
            unique_collaborators=collaboration_data.get("unique_collaborators", 0),
        )

    def _calculate_pr_size(self, pr: PullRequest) -> int:
        """Calculate the total size of a pull request (additions + deletions).

        Args:
            pr: Pull request to calculate size for

        Returns:
            Total number of lines changed
        """
        return pr.additions + pr.deletions

    def _get_total_changes(self, prs: List[PullRequest]) -> int:
        """Calculate total changes across all pull requests.

        Args:
            prs: List of pull requests

        Returns:
            Sum of all additions and deletions
        """
        return sum(self._calculate_pr_size(pr) for pr in prs)

    def _find_largest_pr(self, prs: List[PullRequest]) -> PullRequest:
        """Find the pull request with the most changes.

        Args:
            prs: List of pull requests

        Returns:
            Pull request with the largest number of changes
        """
        return max(prs, key=self._calculate_pr_size)

    def _extract_proudest_moments(self, collection: CollectionResult) -> List[str]:
        """Extract proudest moments from collection data using helper."""
        # Define threshold checks for basic metrics
        basic_checks = [
            (collection.commits, ACTIVITY_THRESHOLDS['very_high_commits'],
             "ì´ {}íšŒì˜ ì»¤ë°‹ìœ¼ë¡œ ê¾¸ì¤€íˆ ì½”ë“œë² ì´ìŠ¤ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤.", (collection.commits,)),
            (collection.pull_requests, ACTIVITY_THRESHOLDS['very_high_prs'],
             "{}ê°œì˜ Pull Requestë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¨¸ì§€í–ˆìŠµë‹ˆë‹¤.", (collection.pull_requests,)),
            (collection.reviews, ACTIVITY_THRESHOLDS['very_high_reviews'],
             "{}íšŒì˜ ì½”ë“œ ë¦¬ë·°ë¡œ íŒ€ì˜ ì½”ë“œ í’ˆì§ˆ í–¥ìƒì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.", (collection.reviews,)),
        ]

        moments = ActivityMessageBuilder.build_messages_from_checks(basic_checks)

        # Add insights from PR examples
        if collection.pull_request_examples:
            total_changes = self._get_total_changes(collection.pull_request_examples)
            msg = ActivityMessageBuilder.build_if_exceeds(
                total_changes,
                ACTIVITY_THRESHOLDS['very_large_pr'],
                "ì´ {:,}ì¤„ì˜ ì½”ë“œ ë³€ê²½ìœ¼ë¡œ ëŒ€ê·œëª¨ ê°œì„ ì„ ì£¼ë„í–ˆìŠµë‹ˆë‹¤.",
                total_changes
            )
            if msg:
                moments.append(msg)

            # Find largest PR
            largest_pr = self._find_largest_pr(collection.pull_request_examples)
            largest_pr_size = self._calculate_pr_size(largest_pr)
            msg = ActivityMessageBuilder.build_if_exceeds(
                largest_pr_size,
                ACTIVITY_THRESHOLDS['large_pr'],
                "ê°€ì¥ í° PR(#{}: {})ì—ì„œ {:,}ì¤„ì˜ ë³€ê²½ìœ¼ë¡œ ë„ì „ì ì¸ ì‘ì—…ì„ ì™„ìˆ˜í–ˆìŠµë‹ˆë‹¤.",
                largest_pr.number, largest_pr.title, largest_pr_size
            )
            if msg:
                moments.append(msg)

        if not moments:
            moments.append("ê¾¸ì¤€í•œ í™œë™ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë°œì „ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.")

        return moments

    def _extract_biggest_challenges(self, collection: CollectionResult) -> List[str]:
        """Extract biggest challenges from collection data."""
        challenges = []
        month_span = max(collection.months, 1)

        if collection.pull_requests > ACTIVITY_THRESHOLDS['high_prs']:
            avg_pr_per_month = collection.pull_requests / month_span
            challenges.append(
                f"ì›”í‰ê·  {avg_pr_per_month:.1f}ê°œì˜ PRì„ ê´€ë¦¬í•˜ë©° ì§€ì†ì ì¸ ë°°í¬ ë¦¬ë“¬ì„ ìœ ì§€í•˜ëŠ” ë„ì „ì„ í•´ëƒˆìŠµë‹ˆë‹¤."
            )

        if collection.reviews > ACTIVITY_THRESHOLDS['high_reviews']:
            challenges.append(
                f"{collection.reviews}íšŒì˜ ì½”ë“œ ë¦¬ë·°ë¥¼ ì§„í–‰í•˜ë©° íŒ€ì›ë“¤ì˜ ë‹¤ì–‘í•œ ê´€ì ì„ ì´í•´í•˜ê³  ì¡°ìœ¨í–ˆìŠµë‹ˆë‹¤."
            )

        if collection.issues > 0:
            challenges.append(
                f"{collection.issues}ê±´ì˜ ì´ìŠˆë¥¼ ì²˜ë¦¬í•˜ë©° ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ê³¼ ìš°ì„ ìˆœìœ„ íŒë‹¨ ëŠ¥ë ¥ì„ í‚¤ì› ìŠµë‹ˆë‹¤."
            )

        # Add PR-specific challenges
        if collection.pull_request_examples:
            msg = InsightExtractor.extract_keyword_based_insight(
                collection.pull_request_examples,
                ['feature', 'feat', 'ê¸°ëŠ¥', 'add'],
                ACTIVITY_THRESHOLDS['feature_pr_threshold'],
                "{count}ê°œì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ê°œë°œí•˜ë©° ìš”êµ¬ì‚¬í•­ ë¶„ì„ê³¼ ì„¤ê³„ ëŠ¥ë ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤."
            )
            if msg:
                challenges.append(msg)

        if not challenges:
            challenges = [
                "ë³µì¡í•œ ê¸°ìˆ ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ë©° ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‚¤ì› ìŠµë‹ˆë‹¤.",
                "íŒ€ì›ë“¤ê³¼ì˜ í˜‘ì—…ì„ í†µí•´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.",
            ]

        return challenges

    def _extract_lessons_learned(self, collection: CollectionResult) -> List[str]:
        """Extract lessons learned from collection data."""
        lessons = []

        if collection.commits > 0 and collection.pull_requests > 0:
            commits_per_pr = collection.commits / collection.pull_requests
            if commits_per_pr > ACTIVITY_THRESHOLDS['high_commits_per_pr']:
                lessons.append(
                    f"PRë‹¹ í‰ê·  {commits_per_pr:.1f}ê°œì˜ ì»¤ë°‹ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. "
                    "ì‘ì€ ë‹¨ìœ„ë¡œ ìì£¼ ì»¤ë°‹í•˜ê³  ë¦¬ë·°ë°›ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
            else:
                lessons.append(
                    f"PRë‹¹ í‰ê·  {commits_per_pr:.1f}ê°œì˜ ì»¤ë°‹ìœ¼ë¡œ ì ì ˆí•œ í¬ê¸°ì˜ ë³€ê²½ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤. "
                    "ì‘ê³  ì§‘ì¤‘ëœ PRì´ ë¦¬ë·°ì™€ ë³‘í•©ì„ ë” ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤."
                )

        if collection.reviews > 0 and collection.pull_requests > 0:
            review_ratio = collection.reviews / collection.pull_requests
            if review_ratio > ACTIVITY_THRESHOLDS['high_review_ratio']:
                lessons.append(
                    f"ë‚´ PRë³´ë‹¤ {review_ratio:.1f}ë°° ë§ì€ ë¦¬ë·°ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. "
                    "ì½”ë“œ ë¦¬ë·°ëŠ” íŒ€ì˜ ì½”ë“œ í’ˆì§ˆì„ ë†’ì´ê³  ì§€ì‹ì„ ê³µìœ í•˜ëŠ” í•µì‹¬ í™œë™ì…ë‹ˆë‹¤."
                )
            else:
                lessons.append(
                    "ì½”ë“œ ë¦¬ë·°ë¥¼ í†µí•´ ë‹¤ë¥¸ íŒ€ì›ë“¤ì˜ ì ‘ê·¼ ë°©ì‹ì„ ë°°ìš°ê³  ì‹œì•¼ë¥¼ ë„“í ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
                )

        if collection.pull_request_examples:
            merged_prs = [pr for pr in collection.pull_request_examples if pr.merged_at]
            if merged_prs:
                merge_rate = len(merged_prs) / len(collection.pull_request_examples)
                if merge_rate > ACTIVITY_THRESHOLDS['high_merge_rate']:
                    lessons.append(
                        f"{merge_rate*100:.0f}%ì˜ ë†’ì€ PR ë¨¸ì§€ìœ¨ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. "
                        "ëª…í™•í•œ ëª©ì ê³¼ ì¶©ë¶„í•œ ì„¤ëª…ì´ ìˆëŠ” PRì´ ì„±ê³µë¥ ì„ ë†’ì…ë‹ˆë‹¤."
                    )

        if not lessons:
            lessons = [
                "ì‘ê³  ìì£¼ ì»¤ë°‹í•˜ëŠ” ê²ƒì´ ì½”ë“œ ë¦¬ë·°ì™€ í˜‘ì—…ì— ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.",
                "ì½”ë“œ ë¦¬ë·°ëŠ” ë‹¨ìˆœí•œ ë²„ê·¸ ì°¾ê¸°ê°€ ì•„ë‹Œ ì§€ì‹ ê³µìœ ì˜ ì¥ì…ë‹ˆë‹¤.",
            ]

        return lessons

    def _extract_next_year_goals(self, collection: CollectionResult) -> List[str]:
        """Extract next year goals from collection data."""
        goals = []

        # Goals based on current weak points
        if collection.reviews < collection.pull_requests:
            goals.append(
                "ì½”ë“œ ë¦¬ë·° ì°¸ì—¬ë¥¼ ëŠ˜ë ¤ íŒ€ì˜ ì½”ë“œ í’ˆì§ˆ í–¥ìƒì— ë”ìš± ê¸°ì—¬í•˜ê¸°"
            )

        if collection.pull_request_examples:
            # Optimize: filter PRs in a single pass to avoid multiple iterations
            pr_categories = InsightExtractor.categorize_prs_by_keywords(
                collection.pull_request_examples,
                {
                    'doc': ['doc', 'readme', 'ë¬¸ì„œ'],
                    'test': ['test', 'í…ŒìŠ¤íŠ¸']
                }
            )

            if len(pr_categories['doc']) < ACTIVITY_THRESHOLDS['moderate_doc_prs']:
                goals.append(
                    "ë¬¸ì„œí™”ì— ë” ì‹ ê²½ì¨ì„œ í”„ë¡œì íŠ¸ì˜ ì ‘ê·¼ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒí•˜ê¸°"
                )

            if len(pr_categories['test']) < ACTIVITY_THRESHOLDS['moderate_test_prs']:
                goals.append(
                    "í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ë¥¼ ë†’ì—¬ ì½”ë“œì˜ ì•ˆì •ì„±ê³¼ ì‹ ë¢°ë„ ê°•í™”í•˜ê¸°"
                )

        # Always include growth goals
        goals.append(
            "ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ í”„ë ˆì„ì›Œí¬ë¥¼ í•™ìŠµí•˜ì—¬ ê¸°ìˆ  ìŠ¤íƒ í™•ì¥í•˜ê¸°"
        )
        goals.append(
            "ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ë‚˜ ê¸°ìˆ  ê³µìœ ë¥¼ í†µí•´ ê°œë°œ ì»¤ë®¤ë‹ˆí‹°ì— í™˜ì›í•˜ê¸°"
        )

        # Limit goals based on configured maximum
        return goals[:DISPLAY_LIMITS['max_goals']]

    def _build_year_end_review(
        self,
        collection: CollectionResult,
        highlights: List[str],
        awards: List[str],
    ) -> YearEndReview:
        """Generate year-end specific review content based on actual data."""
        return YearEndReview(
            proudest_moments=self._extract_proudest_moments(collection),
            biggest_challenges=self._extract_biggest_challenges(collection),
            lessons_learned=self._extract_lessons_learned(collection),
            next_year_goals=self._extract_next_year_goals(collection),
        )
